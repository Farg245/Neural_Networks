{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.10",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "CompanyBanckrupcy.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYyNszjZ_KCO"
      },
      "source": [
        "#imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq-kSS8I-7_M",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2c7c745-0d06-4d29-e1f9-3fac7abfeb06"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAmbch1-hjSu",
        "outputId": "09acf8e8-1298-4f32-f967-ebccba11d911"
      },
      "source": [
        "!pip install optuna"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.7/dist-packages (2.10.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from optuna) (3.13)\n",
            "Requirement already satisfied: scipy!=1.4.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.7/dist-packages (from optuna) (6.6.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from optuna) (1.19.5)\n",
            "Requirement already satisfied: alembic in /usr/local/lib/python3.7/dist-packages (from optuna) (1.7.5)\n",
            "Requirement already satisfied: cliff in /usr/local/lib/python3.7/dist-packages (from optuna) (3.10.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from optuna) (4.62.3)\n",
            "Requirement already satisfied: sqlalchemy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (1.4.27)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from optuna) (21.3)\n",
            "Requirement already satisfied: cmaes>=0.8.2 in /usr/local/lib/python3.7/dist-packages (from optuna) (0.8.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->optuna) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (4.8.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.7/dist-packages (from sqlalchemy>=1.1.0->optuna) (1.1.2)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (5.4.0)\n",
            "Requirement already satisfied: Mako in /usr/local/lib/python3.7/dist-packages (from alembic->optuna) (1.1.6)\n",
            "Requirement already satisfied: autopage>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (0.4.0)\n",
            "Requirement already satisfied: PrettyTable>=0.7.2 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.4.0)\n",
            "Requirement already satisfied: stevedore>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (3.5.0)\n",
            "Requirement already satisfied: pbr!=2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (5.8.0)\n",
            "Requirement already satisfied: cmd2>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from cliff->optuna) (2.3.3)\n",
            "Requirement already satisfied: attrs>=16.3.0 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (21.2.0)\n",
            "Requirement already satisfied: wcwidth>=0.1.7 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (0.2.5)\n",
            "Requirement already satisfied: pyperclip>=1.6 in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (1.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from cmd2>=1.0.0->cliff->optuna) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->sqlalchemy>=1.1.0->optuna) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.7/dist-packages (from Mako->alembic->optuna) (2.0.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.366271Z",
          "iopub.execute_input": "2021-11-25T16:30:54.366522Z",
          "iopub.status.idle": "2021-11-25T16:30:54.548392Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.366494Z",
          "shell.execute_reply": "2021-11-25T16:30:54.547586Z"
        },
        "trusted": true,
        "id": "NU8w8EQh-Qix"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import f1_score, accuracy_score, recall_score, balanced_accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "import pickle\n",
        "import matplotlib.ticker as ticker\n",
        "import optuna\n",
        "from imblearn.over_sampling import SMOTE\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kzq6j1gQ2qZV"
      },
      "source": [
        "#Εισαγωγή datasset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXeKBryw3xmK"
      },
      "source": [
        "Το dataset αφορά εταιρείες της Ασίας και συγκεκριμένα δεδομένα που αντλήθηκαν από το Taiwan Economic Journal την περίοδο του 1999 με 2009. Το χαρακτηριστικό που θα προσπαθήσουμε να προβλέψουμε είναι το εάν η εταιρεία χρεοκόπησε ή οχι."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RpSm-4wq_kDm"
      },
      "source": [
        "file_name = 'drive/MyDrive/NEURAL/first/kaggle/bankruptcy.csv'\n",
        "bankruptcy_df = pd.read_csv(file_name)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.549838Z",
          "iopub.execute_input": "2021-11-25T16:30:54.550148Z",
          "iopub.status.idle": "2021-11-25T16:30:54.575656Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.550108Z",
          "shell.execute_reply": "2021-11-25T16:30:54.574796Z"
        },
        "trusted": true,
        "id": "L_i-8SBE-QjE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 377
        },
        "outputId": "7ac164eb-d6a8-4c21-82c9-2e5aeb7350e1"
      },
      "source": [
        "bankruptcy_df.head()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bankrupt?</th>\n",
              "      <th>ROA(C) before interest and depreciation before interest</th>\n",
              "      <th>ROA(A) before interest and % after tax</th>\n",
              "      <th>ROA(B) before interest and depreciation after tax</th>\n",
              "      <th>Operating Gross Margin</th>\n",
              "      <th>Realized Sales Gross Margin</th>\n",
              "      <th>Operating Profit Rate</th>\n",
              "      <th>Pre-tax net Interest Rate</th>\n",
              "      <th>After-tax net Interest Rate</th>\n",
              "      <th>Non-industry income and expenditure/revenue</th>\n",
              "      <th>Continuous interest rate (after tax)</th>\n",
              "      <th>Operating Expense Rate</th>\n",
              "      <th>Research and development expense rate</th>\n",
              "      <th>Cash flow rate</th>\n",
              "      <th>Interest-bearing debt interest rate</th>\n",
              "      <th>Tax rate (A)</th>\n",
              "      <th>Net Value Per Share (B)</th>\n",
              "      <th>Net Value Per Share (A)</th>\n",
              "      <th>Net Value Per Share (C)</th>\n",
              "      <th>Persistent EPS in the Last Four Seasons</th>\n",
              "      <th>Cash Flow Per Share</th>\n",
              "      <th>Revenue Per Share (Yuan ¥)</th>\n",
              "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
              "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
              "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
              "      <th>Operating Profit Growth Rate</th>\n",
              "      <th>After-tax Net Profit Growth Rate</th>\n",
              "      <th>Regular Net Profit Growth Rate</th>\n",
              "      <th>Continuous Net Profit Growth Rate</th>\n",
              "      <th>Total Asset Growth Rate</th>\n",
              "      <th>Net Value Growth Rate</th>\n",
              "      <th>Total Asset Return Growth Rate Ratio</th>\n",
              "      <th>Cash Reinvestment %</th>\n",
              "      <th>Current Ratio</th>\n",
              "      <th>Quick Ratio</th>\n",
              "      <th>Interest Expense Ratio</th>\n",
              "      <th>Total debt/Total net worth</th>\n",
              "      <th>Debt ratio %</th>\n",
              "      <th>Net worth/Assets</th>\n",
              "      <th>Long-term fund suitability ratio (A)</th>\n",
              "      <th>...</th>\n",
              "      <th>Current Assets/Total Assets</th>\n",
              "      <th>Cash/Total Assets</th>\n",
              "      <th>Quick Assets/Current Liability</th>\n",
              "      <th>Cash/Current Liability</th>\n",
              "      <th>Current Liability to Assets</th>\n",
              "      <th>Operating Funds to Liability</th>\n",
              "      <th>Inventory/Working Capital</th>\n",
              "      <th>Inventory/Current Liability</th>\n",
              "      <th>Current Liabilities/Liability</th>\n",
              "      <th>Working Capital/Equity</th>\n",
              "      <th>Current Liabilities/Equity</th>\n",
              "      <th>Long-term Liability to Current Assets</th>\n",
              "      <th>Retained Earnings to Total Assets</th>\n",
              "      <th>Total income/Total expense</th>\n",
              "      <th>Total expense/Assets</th>\n",
              "      <th>Current Asset Turnover Rate</th>\n",
              "      <th>Quick Asset Turnover Rate</th>\n",
              "      <th>Working capitcal Turnover Rate</th>\n",
              "      <th>Cash Turnover Rate</th>\n",
              "      <th>Cash Flow to Sales</th>\n",
              "      <th>Fixed Assets to Assets</th>\n",
              "      <th>Current Liability to Liability</th>\n",
              "      <th>Current Liability to Equity</th>\n",
              "      <th>Equity to Long-term Liability</th>\n",
              "      <th>Cash Flow to Total Assets</th>\n",
              "      <th>Cash Flow to Liability</th>\n",
              "      <th>CFO to Assets</th>\n",
              "      <th>Cash Flow to Equity</th>\n",
              "      <th>Current Liability to Current Assets</th>\n",
              "      <th>Liability-Assets Flag</th>\n",
              "      <th>Net Income to Total Assets</th>\n",
              "      <th>Total assets to GNP price</th>\n",
              "      <th>No-credit Interval</th>\n",
              "      <th>Gross Profit to Sales</th>\n",
              "      <th>Net Income to Stockholder's Equity</th>\n",
              "      <th>Liability to Equity</th>\n",
              "      <th>Degree of Financial Leverage (DFL)</th>\n",
              "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
              "      <th>Net Income Flag</th>\n",
              "      <th>Equity to Liability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.370594</td>\n",
              "      <td>0.424389</td>\n",
              "      <td>0.405750</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.998969</td>\n",
              "      <td>0.796887</td>\n",
              "      <td>0.808809</td>\n",
              "      <td>0.302646</td>\n",
              "      <td>0.780985</td>\n",
              "      <td>1.256969e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.458143</td>\n",
              "      <td>0.000725</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.169141</td>\n",
              "      <td>0.311664</td>\n",
              "      <td>0.017560</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.138736</td>\n",
              "      <td>0.022102</td>\n",
              "      <td>0.848195</td>\n",
              "      <td>0.688979</td>\n",
              "      <td>0.688979</td>\n",
              "      <td>0.217535</td>\n",
              "      <td>4.980000e+09</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.263100</td>\n",
              "      <td>0.363725</td>\n",
              "      <td>0.002259</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.629951</td>\n",
              "      <td>0.021266</td>\n",
              "      <td>0.207576</td>\n",
              "      <td>0.792424</td>\n",
              "      <td>0.005024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.190643</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>1.473360e-04</td>\n",
              "      <td>0.147308</td>\n",
              "      <td>0.334015</td>\n",
              "      <td>0.276920</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.676269</td>\n",
              "      <td>0.721275</td>\n",
              "      <td>0.339077</td>\n",
              "      <td>0.025592</td>\n",
              "      <td>0.903225</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.064856</td>\n",
              "      <td>7.010000e+08</td>\n",
              "      <td>6.550000e+09</td>\n",
              "      <td>0.593831</td>\n",
              "      <td>4.580000e+08</td>\n",
              "      <td>0.671568</td>\n",
              "      <td>0.424206</td>\n",
              "      <td>0.676269</td>\n",
              "      <td>0.339077</td>\n",
              "      <td>0.126549</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.458609</td>\n",
              "      <td>0.520382</td>\n",
              "      <td>0.312905</td>\n",
              "      <td>0.118250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.716845</td>\n",
              "      <td>0.009219</td>\n",
              "      <td>0.622879</td>\n",
              "      <td>0.601453</td>\n",
              "      <td>0.827890</td>\n",
              "      <td>0.290202</td>\n",
              "      <td>0.026601</td>\n",
              "      <td>0.564050</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.538214</td>\n",
              "      <td>0.516730</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.998946</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809301</td>\n",
              "      <td>0.303556</td>\n",
              "      <td>0.781506</td>\n",
              "      <td>2.897851e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.461867</td>\n",
              "      <td>0.000647</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.208944</td>\n",
              "      <td>0.318137</td>\n",
              "      <td>0.021144</td>\n",
              "      <td>0.093722</td>\n",
              "      <td>0.169918</td>\n",
              "      <td>0.022080</td>\n",
              "      <td>0.848088</td>\n",
              "      <td>0.689693</td>\n",
              "      <td>0.689702</td>\n",
              "      <td>0.217620</td>\n",
              "      <td>6.110000e+09</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.264516</td>\n",
              "      <td>0.376709</td>\n",
              "      <td>0.006016</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.635172</td>\n",
              "      <td>0.012502</td>\n",
              "      <td>0.171176</td>\n",
              "      <td>0.828824</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.182419</td>\n",
              "      <td>0.014948</td>\n",
              "      <td>0.004136</td>\n",
              "      <td>1.383910e-03</td>\n",
              "      <td>0.056963</td>\n",
              "      <td>0.341106</td>\n",
              "      <td>0.289642</td>\n",
              "      <td>0.005210</td>\n",
              "      <td>0.308589</td>\n",
              "      <td>0.731975</td>\n",
              "      <td>0.329740</td>\n",
              "      <td>0.023947</td>\n",
              "      <td>0.931065</td>\n",
              "      <td>0.002226</td>\n",
              "      <td>0.025516</td>\n",
              "      <td>1.065198e-04</td>\n",
              "      <td>7.700000e+09</td>\n",
              "      <td>0.593916</td>\n",
              "      <td>2.490000e+09</td>\n",
              "      <td>0.671570</td>\n",
              "      <td>0.468828</td>\n",
              "      <td>0.308589</td>\n",
              "      <td>0.329740</td>\n",
              "      <td>0.120916</td>\n",
              "      <td>0.641100</td>\n",
              "      <td>0.459001</td>\n",
              "      <td>0.567101</td>\n",
              "      <td>0.314163</td>\n",
              "      <td>0.047775</td>\n",
              "      <td>0</td>\n",
              "      <td>0.795297</td>\n",
              "      <td>0.008323</td>\n",
              "      <td>0.623652</td>\n",
              "      <td>0.610237</td>\n",
              "      <td>0.839969</td>\n",
              "      <td>0.283846</td>\n",
              "      <td>0.264577</td>\n",
              "      <td>0.570175</td>\n",
              "      <td>1</td>\n",
              "      <td>0.020794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.426071</td>\n",
              "      <td>0.499019</td>\n",
              "      <td>0.472295</td>\n",
              "      <td>0.601450</td>\n",
              "      <td>0.601364</td>\n",
              "      <td>0.998857</td>\n",
              "      <td>0.796403</td>\n",
              "      <td>0.808388</td>\n",
              "      <td>0.302035</td>\n",
              "      <td>0.780284</td>\n",
              "      <td>2.361297e-04</td>\n",
              "      <td>25500000.0</td>\n",
              "      <td>0.458521</td>\n",
              "      <td>0.000790</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.177911</td>\n",
              "      <td>0.177911</td>\n",
              "      <td>0.193713</td>\n",
              "      <td>0.180581</td>\n",
              "      <td>0.307102</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>0.092338</td>\n",
              "      <td>0.142803</td>\n",
              "      <td>0.022760</td>\n",
              "      <td>0.848094</td>\n",
              "      <td>0.689463</td>\n",
              "      <td>0.689470</td>\n",
              "      <td>0.217601</td>\n",
              "      <td>7.280000e+09</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.264184</td>\n",
              "      <td>0.368913</td>\n",
              "      <td>0.011543</td>\n",
              "      <td>0.005348</td>\n",
              "      <td>0.629631</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>0.207516</td>\n",
              "      <td>0.792484</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.602806</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.006302</td>\n",
              "      <td>5.340000e+09</td>\n",
              "      <td>0.098162</td>\n",
              "      <td>0.336731</td>\n",
              "      <td>0.277456</td>\n",
              "      <td>0.013879</td>\n",
              "      <td>0.446027</td>\n",
              "      <td>0.742729</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>0.003715</td>\n",
              "      <td>0.909903</td>\n",
              "      <td>0.002060</td>\n",
              "      <td>0.021387</td>\n",
              "      <td>1.791094e-03</td>\n",
              "      <td>1.022676e-03</td>\n",
              "      <td>0.594502</td>\n",
              "      <td>7.610000e+08</td>\n",
              "      <td>0.671571</td>\n",
              "      <td>0.276179</td>\n",
              "      <td>0.446027</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>0.117922</td>\n",
              "      <td>0.642765</td>\n",
              "      <td>0.459254</td>\n",
              "      <td>0.538491</td>\n",
              "      <td>0.314515</td>\n",
              "      <td>0.025346</td>\n",
              "      <td>0</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.040003</td>\n",
              "      <td>0.623841</td>\n",
              "      <td>0.601449</td>\n",
              "      <td>0.836774</td>\n",
              "      <td>0.290189</td>\n",
              "      <td>0.026555</td>\n",
              "      <td>0.563706</td>\n",
              "      <td>1</td>\n",
              "      <td>0.016474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.399844</td>\n",
              "      <td>0.451265</td>\n",
              "      <td>0.457733</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.998700</td>\n",
              "      <td>0.796967</td>\n",
              "      <td>0.808966</td>\n",
              "      <td>0.303350</td>\n",
              "      <td>0.781241</td>\n",
              "      <td>1.078888e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.465705</td>\n",
              "      <td>0.000449</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.193722</td>\n",
              "      <td>0.321674</td>\n",
              "      <td>0.014368</td>\n",
              "      <td>0.077762</td>\n",
              "      <td>0.148603</td>\n",
              "      <td>0.022046</td>\n",
              "      <td>0.848005</td>\n",
              "      <td>0.689110</td>\n",
              "      <td>0.689110</td>\n",
              "      <td>0.217568</td>\n",
              "      <td>4.880000e+09</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.263371</td>\n",
              "      <td>0.384077</td>\n",
              "      <td>0.004194</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.630228</td>\n",
              "      <td>0.009572</td>\n",
              "      <td>0.151465</td>\n",
              "      <td>0.848535</td>\n",
              "      <td>0.005047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.225815</td>\n",
              "      <td>0.018851</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>1.010646e-03</td>\n",
              "      <td>0.098715</td>\n",
              "      <td>0.348716</td>\n",
              "      <td>0.276580</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>0.615848</td>\n",
              "      <td>0.729825</td>\n",
              "      <td>0.331509</td>\n",
              "      <td>0.022165</td>\n",
              "      <td>0.906902</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.024161</td>\n",
              "      <td>8.140000e+09</td>\n",
              "      <td>6.050000e+09</td>\n",
              "      <td>0.593889</td>\n",
              "      <td>2.030000e+09</td>\n",
              "      <td>0.671519</td>\n",
              "      <td>0.559144</td>\n",
              "      <td>0.615848</td>\n",
              "      <td>0.331509</td>\n",
              "      <td>0.120760</td>\n",
              "      <td>0.579039</td>\n",
              "      <td>0.448518</td>\n",
              "      <td>0.604105</td>\n",
              "      <td>0.302382</td>\n",
              "      <td>0.067250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.739555</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>0.622929</td>\n",
              "      <td>0.583538</td>\n",
              "      <td>0.834697</td>\n",
              "      <td>0.281721</td>\n",
              "      <td>0.026697</td>\n",
              "      <td>0.564663</td>\n",
              "      <td>1</td>\n",
              "      <td>0.023982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.465022</td>\n",
              "      <td>0.538432</td>\n",
              "      <td>0.522298</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809304</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>0.781550</td>\n",
              "      <td>7.890000e+09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.462746</td>\n",
              "      <td>0.000686</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.212537</td>\n",
              "      <td>0.319162</td>\n",
              "      <td>0.029690</td>\n",
              "      <td>0.096898</td>\n",
              "      <td>0.168412</td>\n",
              "      <td>0.022096</td>\n",
              "      <td>0.848258</td>\n",
              "      <td>0.689697</td>\n",
              "      <td>0.689697</td>\n",
              "      <td>0.217626</td>\n",
              "      <td>5.510000e+09</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.265218</td>\n",
              "      <td>0.379690</td>\n",
              "      <td>0.006022</td>\n",
              "      <td>0.003727</td>\n",
              "      <td>0.636055</td>\n",
              "      <td>0.005150</td>\n",
              "      <td>0.106509</td>\n",
              "      <td>0.893491</td>\n",
              "      <td>0.005303</td>\n",
              "      <td>...</td>\n",
              "      <td>0.358380</td>\n",
              "      <td>0.014161</td>\n",
              "      <td>0.004275</td>\n",
              "      <td>6.804636e-04</td>\n",
              "      <td>0.110195</td>\n",
              "      <td>0.344639</td>\n",
              "      <td>0.287913</td>\n",
              "      <td>0.004869</td>\n",
              "      <td>0.975007</td>\n",
              "      <td>0.732000</td>\n",
              "      <td>0.330726</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.913850</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.026385</td>\n",
              "      <td>6.680000e+09</td>\n",
              "      <td>5.050000e+09</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>8.240000e+08</td>\n",
              "      <td>0.671563</td>\n",
              "      <td>0.309555</td>\n",
              "      <td>0.975007</td>\n",
              "      <td>0.330726</td>\n",
              "      <td>0.110933</td>\n",
              "      <td>0.622374</td>\n",
              "      <td>0.454411</td>\n",
              "      <td>0.578469</td>\n",
              "      <td>0.311567</td>\n",
              "      <td>0.047725</td>\n",
              "      <td>0</td>\n",
              "      <td>0.795016</td>\n",
              "      <td>0.003878</td>\n",
              "      <td>0.623521</td>\n",
              "      <td>0.598782</td>\n",
              "      <td>0.839973</td>\n",
              "      <td>0.278514</td>\n",
              "      <td>0.024752</td>\n",
              "      <td>0.575617</td>\n",
              "      <td>1</td>\n",
              "      <td>0.035490</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 96 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Bankrupt?  ...   Equity to Liability\n",
              "0          1  ...              0.016469\n",
              "1          1  ...              0.020794\n",
              "2          1  ...              0.016474\n",
              "3          1  ...              0.023982\n",
              "4          1  ...              0.035490\n",
              "\n",
              "[5 rows x 96 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tewKeg9vR1ms"
      },
      "source": [
        "Είδαμε στο dataset ότι το  Net Income Flag έχει σταθερή τιμή 1, οπότε το κάνουμε drop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oQaqfmEsQMtC",
        "outputId": "3d9715fc-f48d-4f8f-f424-2c1ad01199d5"
      },
      "source": [
        "bankruptcy_df[' Net Income Flag'].value_counts()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    6819\n",
              "Name:  Net Income Flag, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 565
        },
        "id": "78uYeAbDRrdu",
        "outputId": "7fe6a792-60c6-4e46-ff7a-0391fd8b397a"
      },
      "source": [
        "bankruptcy_df.drop([' Net Income Flag'], axis=1)"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Bankrupt?</th>\n",
              "      <th>ROA(C) before interest and depreciation before interest</th>\n",
              "      <th>ROA(A) before interest and % after tax</th>\n",
              "      <th>ROA(B) before interest and depreciation after tax</th>\n",
              "      <th>Operating Gross Margin</th>\n",
              "      <th>Realized Sales Gross Margin</th>\n",
              "      <th>Operating Profit Rate</th>\n",
              "      <th>Pre-tax net Interest Rate</th>\n",
              "      <th>After-tax net Interest Rate</th>\n",
              "      <th>Non-industry income and expenditure/revenue</th>\n",
              "      <th>Continuous interest rate (after tax)</th>\n",
              "      <th>Operating Expense Rate</th>\n",
              "      <th>Research and development expense rate</th>\n",
              "      <th>Cash flow rate</th>\n",
              "      <th>Interest-bearing debt interest rate</th>\n",
              "      <th>Tax rate (A)</th>\n",
              "      <th>Net Value Per Share (B)</th>\n",
              "      <th>Net Value Per Share (A)</th>\n",
              "      <th>Net Value Per Share (C)</th>\n",
              "      <th>Persistent EPS in the Last Four Seasons</th>\n",
              "      <th>Cash Flow Per Share</th>\n",
              "      <th>Revenue Per Share (Yuan ¥)</th>\n",
              "      <th>Operating Profit Per Share (Yuan ¥)</th>\n",
              "      <th>Per Share Net profit before tax (Yuan ¥)</th>\n",
              "      <th>Realized Sales Gross Profit Growth Rate</th>\n",
              "      <th>Operating Profit Growth Rate</th>\n",
              "      <th>After-tax Net Profit Growth Rate</th>\n",
              "      <th>Regular Net Profit Growth Rate</th>\n",
              "      <th>Continuous Net Profit Growth Rate</th>\n",
              "      <th>Total Asset Growth Rate</th>\n",
              "      <th>Net Value Growth Rate</th>\n",
              "      <th>Total Asset Return Growth Rate Ratio</th>\n",
              "      <th>Cash Reinvestment %</th>\n",
              "      <th>Current Ratio</th>\n",
              "      <th>Quick Ratio</th>\n",
              "      <th>Interest Expense Ratio</th>\n",
              "      <th>Total debt/Total net worth</th>\n",
              "      <th>Debt ratio %</th>\n",
              "      <th>Net worth/Assets</th>\n",
              "      <th>Long-term fund suitability ratio (A)</th>\n",
              "      <th>...</th>\n",
              "      <th>Quick Assets/Total Assets</th>\n",
              "      <th>Current Assets/Total Assets</th>\n",
              "      <th>Cash/Total Assets</th>\n",
              "      <th>Quick Assets/Current Liability</th>\n",
              "      <th>Cash/Current Liability</th>\n",
              "      <th>Current Liability to Assets</th>\n",
              "      <th>Operating Funds to Liability</th>\n",
              "      <th>Inventory/Working Capital</th>\n",
              "      <th>Inventory/Current Liability</th>\n",
              "      <th>Current Liabilities/Liability</th>\n",
              "      <th>Working Capital/Equity</th>\n",
              "      <th>Current Liabilities/Equity</th>\n",
              "      <th>Long-term Liability to Current Assets</th>\n",
              "      <th>Retained Earnings to Total Assets</th>\n",
              "      <th>Total income/Total expense</th>\n",
              "      <th>Total expense/Assets</th>\n",
              "      <th>Current Asset Turnover Rate</th>\n",
              "      <th>Quick Asset Turnover Rate</th>\n",
              "      <th>Working capitcal Turnover Rate</th>\n",
              "      <th>Cash Turnover Rate</th>\n",
              "      <th>Cash Flow to Sales</th>\n",
              "      <th>Fixed Assets to Assets</th>\n",
              "      <th>Current Liability to Liability</th>\n",
              "      <th>Current Liability to Equity</th>\n",
              "      <th>Equity to Long-term Liability</th>\n",
              "      <th>Cash Flow to Total Assets</th>\n",
              "      <th>Cash Flow to Liability</th>\n",
              "      <th>CFO to Assets</th>\n",
              "      <th>Cash Flow to Equity</th>\n",
              "      <th>Current Liability to Current Assets</th>\n",
              "      <th>Liability-Assets Flag</th>\n",
              "      <th>Net Income to Total Assets</th>\n",
              "      <th>Total assets to GNP price</th>\n",
              "      <th>No-credit Interval</th>\n",
              "      <th>Gross Profit to Sales</th>\n",
              "      <th>Net Income to Stockholder's Equity</th>\n",
              "      <th>Liability to Equity</th>\n",
              "      <th>Degree of Financial Leverage (DFL)</th>\n",
              "      <th>Interest Coverage Ratio (Interest expense to EBIT)</th>\n",
              "      <th>Equity to Liability</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0.370594</td>\n",
              "      <td>0.424389</td>\n",
              "      <td>0.405750</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.601457</td>\n",
              "      <td>0.998969</td>\n",
              "      <td>0.796887</td>\n",
              "      <td>0.808809</td>\n",
              "      <td>0.302646</td>\n",
              "      <td>0.780985</td>\n",
              "      <td>1.256969e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.458143</td>\n",
              "      <td>7.250725e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.147950</td>\n",
              "      <td>0.169141</td>\n",
              "      <td>0.311664</td>\n",
              "      <td>0.017560</td>\n",
              "      <td>0.095921</td>\n",
              "      <td>0.138736</td>\n",
              "      <td>0.022102</td>\n",
              "      <td>0.848195</td>\n",
              "      <td>0.688979</td>\n",
              "      <td>0.688979</td>\n",
              "      <td>0.217535</td>\n",
              "      <td>4.980000e+09</td>\n",
              "      <td>0.000327</td>\n",
              "      <td>0.263100</td>\n",
              "      <td>0.363725</td>\n",
              "      <td>0.002259</td>\n",
              "      <td>0.001208</td>\n",
              "      <td>0.629951</td>\n",
              "      <td>0.021266</td>\n",
              "      <td>0.207576</td>\n",
              "      <td>0.792424</td>\n",
              "      <td>0.005024</td>\n",
              "      <td>...</td>\n",
              "      <td>0.166673</td>\n",
              "      <td>0.190643</td>\n",
              "      <td>0.004094</td>\n",
              "      <td>0.001997</td>\n",
              "      <td>1.473360e-04</td>\n",
              "      <td>0.147308</td>\n",
              "      <td>0.334015</td>\n",
              "      <td>0.276920</td>\n",
              "      <td>0.001036</td>\n",
              "      <td>0.676269</td>\n",
              "      <td>0.721275</td>\n",
              "      <td>0.339077</td>\n",
              "      <td>2.559237e-02</td>\n",
              "      <td>0.903225</td>\n",
              "      <td>0.002022</td>\n",
              "      <td>0.064856</td>\n",
              "      <td>7.010000e+08</td>\n",
              "      <td>6.550000e+09</td>\n",
              "      <td>0.593831</td>\n",
              "      <td>4.580000e+08</td>\n",
              "      <td>0.671568</td>\n",
              "      <td>0.424206</td>\n",
              "      <td>0.676269</td>\n",
              "      <td>0.339077</td>\n",
              "      <td>0.126549</td>\n",
              "      <td>0.637555</td>\n",
              "      <td>0.458609</td>\n",
              "      <td>0.520382</td>\n",
              "      <td>0.312905</td>\n",
              "      <td>0.118250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.716845</td>\n",
              "      <td>0.009219</td>\n",
              "      <td>0.622879</td>\n",
              "      <td>0.601453</td>\n",
              "      <td>0.827890</td>\n",
              "      <td>0.290202</td>\n",
              "      <td>0.026601</td>\n",
              "      <td>0.564050</td>\n",
              "      <td>0.016469</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.464291</td>\n",
              "      <td>0.538214</td>\n",
              "      <td>0.516730</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.610235</td>\n",
              "      <td>0.998946</td>\n",
              "      <td>0.797380</td>\n",
              "      <td>0.809301</td>\n",
              "      <td>0.303556</td>\n",
              "      <td>0.781506</td>\n",
              "      <td>2.897851e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.461867</td>\n",
              "      <td>6.470647e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.182251</td>\n",
              "      <td>0.208944</td>\n",
              "      <td>0.318137</td>\n",
              "      <td>0.021144</td>\n",
              "      <td>0.093722</td>\n",
              "      <td>0.169918</td>\n",
              "      <td>0.022080</td>\n",
              "      <td>0.848088</td>\n",
              "      <td>0.689693</td>\n",
              "      <td>0.689702</td>\n",
              "      <td>0.217620</td>\n",
              "      <td>6.110000e+09</td>\n",
              "      <td>0.000443</td>\n",
              "      <td>0.264516</td>\n",
              "      <td>0.376709</td>\n",
              "      <td>0.006016</td>\n",
              "      <td>0.004039</td>\n",
              "      <td>0.635172</td>\n",
              "      <td>0.012502</td>\n",
              "      <td>0.171176</td>\n",
              "      <td>0.828824</td>\n",
              "      <td>0.005059</td>\n",
              "      <td>...</td>\n",
              "      <td>0.127236</td>\n",
              "      <td>0.182419</td>\n",
              "      <td>0.014948</td>\n",
              "      <td>0.004136</td>\n",
              "      <td>1.383910e-03</td>\n",
              "      <td>0.056963</td>\n",
              "      <td>0.341106</td>\n",
              "      <td>0.289642</td>\n",
              "      <td>0.005210</td>\n",
              "      <td>0.308589</td>\n",
              "      <td>0.731975</td>\n",
              "      <td>0.329740</td>\n",
              "      <td>2.394682e-02</td>\n",
              "      <td>0.931065</td>\n",
              "      <td>0.002226</td>\n",
              "      <td>0.025516</td>\n",
              "      <td>1.065198e-04</td>\n",
              "      <td>7.700000e+09</td>\n",
              "      <td>0.593916</td>\n",
              "      <td>2.490000e+09</td>\n",
              "      <td>0.671570</td>\n",
              "      <td>0.468828</td>\n",
              "      <td>0.308589</td>\n",
              "      <td>0.329740</td>\n",
              "      <td>0.120916</td>\n",
              "      <td>0.641100</td>\n",
              "      <td>0.459001</td>\n",
              "      <td>0.567101</td>\n",
              "      <td>0.314163</td>\n",
              "      <td>0.047775</td>\n",
              "      <td>0</td>\n",
              "      <td>0.795297</td>\n",
              "      <td>0.008323</td>\n",
              "      <td>0.623652</td>\n",
              "      <td>0.610237</td>\n",
              "      <td>0.839969</td>\n",
              "      <td>0.283846</td>\n",
              "      <td>0.264577</td>\n",
              "      <td>0.570175</td>\n",
              "      <td>0.020794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0.426071</td>\n",
              "      <td>0.499019</td>\n",
              "      <td>0.472295</td>\n",
              "      <td>0.601450</td>\n",
              "      <td>0.601364</td>\n",
              "      <td>0.998857</td>\n",
              "      <td>0.796403</td>\n",
              "      <td>0.808388</td>\n",
              "      <td>0.302035</td>\n",
              "      <td>0.780284</td>\n",
              "      <td>2.361297e-04</td>\n",
              "      <td>2.550000e+07</td>\n",
              "      <td>0.458521</td>\n",
              "      <td>7.900790e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.177911</td>\n",
              "      <td>0.177911</td>\n",
              "      <td>0.193713</td>\n",
              "      <td>0.180581</td>\n",
              "      <td>0.307102</td>\n",
              "      <td>0.005944</td>\n",
              "      <td>0.092338</td>\n",
              "      <td>0.142803</td>\n",
              "      <td>0.022760</td>\n",
              "      <td>0.848094</td>\n",
              "      <td>0.689463</td>\n",
              "      <td>0.689470</td>\n",
              "      <td>0.217601</td>\n",
              "      <td>7.280000e+09</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.264184</td>\n",
              "      <td>0.368913</td>\n",
              "      <td>0.011543</td>\n",
              "      <td>0.005348</td>\n",
              "      <td>0.629631</td>\n",
              "      <td>0.021248</td>\n",
              "      <td>0.207516</td>\n",
              "      <td>0.792484</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340201</td>\n",
              "      <td>0.602806</td>\n",
              "      <td>0.000991</td>\n",
              "      <td>0.006302</td>\n",
              "      <td>5.340000e+09</td>\n",
              "      <td>0.098162</td>\n",
              "      <td>0.336731</td>\n",
              "      <td>0.277456</td>\n",
              "      <td>0.013879</td>\n",
              "      <td>0.446027</td>\n",
              "      <td>0.742729</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>3.715116e-03</td>\n",
              "      <td>0.909903</td>\n",
              "      <td>0.002060</td>\n",
              "      <td>0.021387</td>\n",
              "      <td>1.791094e-03</td>\n",
              "      <td>1.022676e-03</td>\n",
              "      <td>0.594502</td>\n",
              "      <td>7.610000e+08</td>\n",
              "      <td>0.671571</td>\n",
              "      <td>0.276179</td>\n",
              "      <td>0.446027</td>\n",
              "      <td>0.334777</td>\n",
              "      <td>0.117922</td>\n",
              "      <td>0.642765</td>\n",
              "      <td>0.459254</td>\n",
              "      <td>0.538491</td>\n",
              "      <td>0.314515</td>\n",
              "      <td>0.025346</td>\n",
              "      <td>0</td>\n",
              "      <td>0.774670</td>\n",
              "      <td>0.040003</td>\n",
              "      <td>0.623841</td>\n",
              "      <td>0.601449</td>\n",
              "      <td>0.836774</td>\n",
              "      <td>0.290189</td>\n",
              "      <td>0.026555</td>\n",
              "      <td>0.563706</td>\n",
              "      <td>0.016474</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0.399844</td>\n",
              "      <td>0.451265</td>\n",
              "      <td>0.457733</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.583541</td>\n",
              "      <td>0.998700</td>\n",
              "      <td>0.796967</td>\n",
              "      <td>0.808966</td>\n",
              "      <td>0.303350</td>\n",
              "      <td>0.781241</td>\n",
              "      <td>1.078888e-04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.465705</td>\n",
              "      <td>4.490449e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.154187</td>\n",
              "      <td>0.193722</td>\n",
              "      <td>0.321674</td>\n",
              "      <td>0.014368</td>\n",
              "      <td>0.077762</td>\n",
              "      <td>0.148603</td>\n",
              "      <td>0.022046</td>\n",
              "      <td>0.848005</td>\n",
              "      <td>0.689110</td>\n",
              "      <td>0.689110</td>\n",
              "      <td>0.217568</td>\n",
              "      <td>4.880000e+09</td>\n",
              "      <td>0.000382</td>\n",
              "      <td>0.263371</td>\n",
              "      <td>0.384077</td>\n",
              "      <td>0.004194</td>\n",
              "      <td>0.002896</td>\n",
              "      <td>0.630228</td>\n",
              "      <td>0.009572</td>\n",
              "      <td>0.151465</td>\n",
              "      <td>0.848535</td>\n",
              "      <td>0.005047</td>\n",
              "      <td>...</td>\n",
              "      <td>0.161575</td>\n",
              "      <td>0.225815</td>\n",
              "      <td>0.018851</td>\n",
              "      <td>0.002961</td>\n",
              "      <td>1.010646e-03</td>\n",
              "      <td>0.098715</td>\n",
              "      <td>0.348716</td>\n",
              "      <td>0.276580</td>\n",
              "      <td>0.003540</td>\n",
              "      <td>0.615848</td>\n",
              "      <td>0.729825</td>\n",
              "      <td>0.331509</td>\n",
              "      <td>2.216520e-02</td>\n",
              "      <td>0.906902</td>\n",
              "      <td>0.001831</td>\n",
              "      <td>0.024161</td>\n",
              "      <td>8.140000e+09</td>\n",
              "      <td>6.050000e+09</td>\n",
              "      <td>0.593889</td>\n",
              "      <td>2.030000e+09</td>\n",
              "      <td>0.671519</td>\n",
              "      <td>0.559144</td>\n",
              "      <td>0.615848</td>\n",
              "      <td>0.331509</td>\n",
              "      <td>0.120760</td>\n",
              "      <td>0.579039</td>\n",
              "      <td>0.448518</td>\n",
              "      <td>0.604105</td>\n",
              "      <td>0.302382</td>\n",
              "      <td>0.067250</td>\n",
              "      <td>0</td>\n",
              "      <td>0.739555</td>\n",
              "      <td>0.003252</td>\n",
              "      <td>0.622929</td>\n",
              "      <td>0.583538</td>\n",
              "      <td>0.834697</td>\n",
              "      <td>0.281721</td>\n",
              "      <td>0.026697</td>\n",
              "      <td>0.564663</td>\n",
              "      <td>0.023982</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>0.465022</td>\n",
              "      <td>0.538432</td>\n",
              "      <td>0.522298</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.598783</td>\n",
              "      <td>0.998973</td>\n",
              "      <td>0.797366</td>\n",
              "      <td>0.809304</td>\n",
              "      <td>0.303475</td>\n",
              "      <td>0.781550</td>\n",
              "      <td>7.890000e+09</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.462746</td>\n",
              "      <td>6.860686e-04</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.167502</td>\n",
              "      <td>0.212537</td>\n",
              "      <td>0.319162</td>\n",
              "      <td>0.029690</td>\n",
              "      <td>0.096898</td>\n",
              "      <td>0.168412</td>\n",
              "      <td>0.022096</td>\n",
              "      <td>0.848258</td>\n",
              "      <td>0.689697</td>\n",
              "      <td>0.689697</td>\n",
              "      <td>0.217626</td>\n",
              "      <td>5.510000e+09</td>\n",
              "      <td>0.000439</td>\n",
              "      <td>0.265218</td>\n",
              "      <td>0.379690</td>\n",
              "      <td>0.006022</td>\n",
              "      <td>0.003727</td>\n",
              "      <td>0.636055</td>\n",
              "      <td>0.005150</td>\n",
              "      <td>0.106509</td>\n",
              "      <td>0.893491</td>\n",
              "      <td>0.005303</td>\n",
              "      <td>...</td>\n",
              "      <td>0.260330</td>\n",
              "      <td>0.358380</td>\n",
              "      <td>0.014161</td>\n",
              "      <td>0.004275</td>\n",
              "      <td>6.804636e-04</td>\n",
              "      <td>0.110195</td>\n",
              "      <td>0.344639</td>\n",
              "      <td>0.287913</td>\n",
              "      <td>0.004869</td>\n",
              "      <td>0.975007</td>\n",
              "      <td>0.732000</td>\n",
              "      <td>0.330726</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.913850</td>\n",
              "      <td>0.002224</td>\n",
              "      <td>0.026385</td>\n",
              "      <td>6.680000e+09</td>\n",
              "      <td>5.050000e+09</td>\n",
              "      <td>0.593915</td>\n",
              "      <td>8.240000e+08</td>\n",
              "      <td>0.671563</td>\n",
              "      <td>0.309555</td>\n",
              "      <td>0.975007</td>\n",
              "      <td>0.330726</td>\n",
              "      <td>0.110933</td>\n",
              "      <td>0.622374</td>\n",
              "      <td>0.454411</td>\n",
              "      <td>0.578469</td>\n",
              "      <td>0.311567</td>\n",
              "      <td>0.047725</td>\n",
              "      <td>0</td>\n",
              "      <td>0.795016</td>\n",
              "      <td>0.003878</td>\n",
              "      <td>0.623521</td>\n",
              "      <td>0.598782</td>\n",
              "      <td>0.839973</td>\n",
              "      <td>0.278514</td>\n",
              "      <td>0.024752</td>\n",
              "      <td>0.575617</td>\n",
              "      <td>0.035490</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6814</th>\n",
              "      <td>0</td>\n",
              "      <td>0.493687</td>\n",
              "      <td>0.539468</td>\n",
              "      <td>0.543230</td>\n",
              "      <td>0.604455</td>\n",
              "      <td>0.604462</td>\n",
              "      <td>0.998992</td>\n",
              "      <td>0.797409</td>\n",
              "      <td>0.809331</td>\n",
              "      <td>0.303510</td>\n",
              "      <td>0.781588</td>\n",
              "      <td>1.510213e-04</td>\n",
              "      <td>4.500000e+09</td>\n",
              "      <td>0.463734</td>\n",
              "      <td>1.790179e-04</td>\n",
              "      <td>0.113372</td>\n",
              "      <td>0.175045</td>\n",
              "      <td>0.175045</td>\n",
              "      <td>0.175045</td>\n",
              "      <td>0.216602</td>\n",
              "      <td>0.320966</td>\n",
              "      <td>0.020766</td>\n",
              "      <td>0.098200</td>\n",
              "      <td>0.172102</td>\n",
              "      <td>0.022374</td>\n",
              "      <td>0.848205</td>\n",
              "      <td>0.689778</td>\n",
              "      <td>0.689778</td>\n",
              "      <td>0.217635</td>\n",
              "      <td>7.070000e+09</td>\n",
              "      <td>0.000450</td>\n",
              "      <td>0.264517</td>\n",
              "      <td>0.380155</td>\n",
              "      <td>0.010451</td>\n",
              "      <td>0.005457</td>\n",
              "      <td>0.631415</td>\n",
              "      <td>0.006655</td>\n",
              "      <td>0.124618</td>\n",
              "      <td>0.875382</td>\n",
              "      <td>0.005150</td>\n",
              "      <td>...</td>\n",
              "      <td>0.312840</td>\n",
              "      <td>0.578455</td>\n",
              "      <td>0.099481</td>\n",
              "      <td>0.005469</td>\n",
              "      <td>5.071548e-03</td>\n",
              "      <td>0.103838</td>\n",
              "      <td>0.346224</td>\n",
              "      <td>0.277543</td>\n",
              "      <td>0.013212</td>\n",
              "      <td>0.786888</td>\n",
              "      <td>0.736716</td>\n",
              "      <td>0.330914</td>\n",
              "      <td>1.792237e-03</td>\n",
              "      <td>0.925611</td>\n",
              "      <td>0.002266</td>\n",
              "      <td>0.019060</td>\n",
              "      <td>2.294154e-04</td>\n",
              "      <td>1.244230e-04</td>\n",
              "      <td>0.593985</td>\n",
              "      <td>1.077940e-04</td>\n",
              "      <td>0.671570</td>\n",
              "      <td>0.400338</td>\n",
              "      <td>0.786888</td>\n",
              "      <td>0.330914</td>\n",
              "      <td>0.112622</td>\n",
              "      <td>0.639806</td>\n",
              "      <td>0.458639</td>\n",
              "      <td>0.587178</td>\n",
              "      <td>0.314063</td>\n",
              "      <td>0.027951</td>\n",
              "      <td>0</td>\n",
              "      <td>0.799927</td>\n",
              "      <td>0.000466</td>\n",
              "      <td>0.623620</td>\n",
              "      <td>0.604455</td>\n",
              "      <td>0.840359</td>\n",
              "      <td>0.279606</td>\n",
              "      <td>0.027064</td>\n",
              "      <td>0.566193</td>\n",
              "      <td>0.029890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6815</th>\n",
              "      <td>0</td>\n",
              "      <td>0.475162</td>\n",
              "      <td>0.538269</td>\n",
              "      <td>0.524172</td>\n",
              "      <td>0.598308</td>\n",
              "      <td>0.598308</td>\n",
              "      <td>0.998992</td>\n",
              "      <td>0.797414</td>\n",
              "      <td>0.809327</td>\n",
              "      <td>0.303520</td>\n",
              "      <td>0.781586</td>\n",
              "      <td>5.220000e+09</td>\n",
              "      <td>1.440000e+09</td>\n",
              "      <td>0.461978</td>\n",
              "      <td>2.370237e-04</td>\n",
              "      <td>0.371596</td>\n",
              "      <td>0.181324</td>\n",
              "      <td>0.181324</td>\n",
              "      <td>0.181324</td>\n",
              "      <td>0.216697</td>\n",
              "      <td>0.318278</td>\n",
              "      <td>0.023050</td>\n",
              "      <td>0.098608</td>\n",
              "      <td>0.172780</td>\n",
              "      <td>0.022159</td>\n",
              "      <td>0.848245</td>\n",
              "      <td>0.689734</td>\n",
              "      <td>0.689734</td>\n",
              "      <td>0.217631</td>\n",
              "      <td>5.220000e+09</td>\n",
              "      <td>0.000445</td>\n",
              "      <td>0.264730</td>\n",
              "      <td>0.377389</td>\n",
              "      <td>0.009259</td>\n",
              "      <td>0.006741</td>\n",
              "      <td>0.631489</td>\n",
              "      <td>0.004623</td>\n",
              "      <td>0.099253</td>\n",
              "      <td>0.900747</td>\n",
              "      <td>0.006772</td>\n",
              "      <td>...</td>\n",
              "      <td>0.335085</td>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.080337</td>\n",
              "      <td>0.006790</td>\n",
              "      <td>4.727181e-03</td>\n",
              "      <td>0.089901</td>\n",
              "      <td>0.342166</td>\n",
              "      <td>0.277368</td>\n",
              "      <td>0.006730</td>\n",
              "      <td>0.849898</td>\n",
              "      <td>0.734584</td>\n",
              "      <td>0.329753</td>\n",
              "      <td>2.204673e-03</td>\n",
              "      <td>0.932629</td>\n",
              "      <td>0.002288</td>\n",
              "      <td>0.011118</td>\n",
              "      <td>1.517299e-04</td>\n",
              "      <td>1.173396e-04</td>\n",
              "      <td>0.593954</td>\n",
              "      <td>7.710000e+09</td>\n",
              "      <td>0.671572</td>\n",
              "      <td>0.096136</td>\n",
              "      <td>0.849898</td>\n",
              "      <td>0.329753</td>\n",
              "      <td>0.112329</td>\n",
              "      <td>0.642072</td>\n",
              "      <td>0.459058</td>\n",
              "      <td>0.569498</td>\n",
              "      <td>0.314446</td>\n",
              "      <td>0.031470</td>\n",
              "      <td>0</td>\n",
              "      <td>0.799748</td>\n",
              "      <td>0.001959</td>\n",
              "      <td>0.623931</td>\n",
              "      <td>0.598306</td>\n",
              "      <td>0.840306</td>\n",
              "      <td>0.278132</td>\n",
              "      <td>0.027009</td>\n",
              "      <td>0.566018</td>\n",
              "      <td>0.038284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6816</th>\n",
              "      <td>0</td>\n",
              "      <td>0.472725</td>\n",
              "      <td>0.533744</td>\n",
              "      <td>0.520638</td>\n",
              "      <td>0.610444</td>\n",
              "      <td>0.610213</td>\n",
              "      <td>0.998984</td>\n",
              "      <td>0.797401</td>\n",
              "      <td>0.809317</td>\n",
              "      <td>0.303512</td>\n",
              "      <td>0.781546</td>\n",
              "      <td>2.509312e-04</td>\n",
              "      <td>1.039086e-04</td>\n",
              "      <td>0.472189</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.490839</td>\n",
              "      <td>0.269521</td>\n",
              "      <td>0.269521</td>\n",
              "      <td>0.269521</td>\n",
              "      <td>0.210929</td>\n",
              "      <td>0.324857</td>\n",
              "      <td>0.044255</td>\n",
              "      <td>0.100073</td>\n",
              "      <td>0.173232</td>\n",
              "      <td>0.022068</td>\n",
              "      <td>0.847978</td>\n",
              "      <td>0.689202</td>\n",
              "      <td>0.689202</td>\n",
              "      <td>0.217547</td>\n",
              "      <td>5.990000e+09</td>\n",
              "      <td>0.000435</td>\n",
              "      <td>0.263858</td>\n",
              "      <td>0.379392</td>\n",
              "      <td>0.038424</td>\n",
              "      <td>0.035112</td>\n",
              "      <td>0.630612</td>\n",
              "      <td>0.001392</td>\n",
              "      <td>0.038939</td>\n",
              "      <td>0.961061</td>\n",
              "      <td>0.009149</td>\n",
              "      <td>...</td>\n",
              "      <td>0.476747</td>\n",
              "      <td>0.496053</td>\n",
              "      <td>0.412885</td>\n",
              "      <td>0.035531</td>\n",
              "      <td>8.821248e-02</td>\n",
              "      <td>0.024414</td>\n",
              "      <td>0.358847</td>\n",
              "      <td>0.277022</td>\n",
              "      <td>0.007810</td>\n",
              "      <td>0.553964</td>\n",
              "      <td>0.737432</td>\n",
              "      <td>0.326921</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.932000</td>\n",
              "      <td>0.002239</td>\n",
              "      <td>0.035446</td>\n",
              "      <td>1.762272e-04</td>\n",
              "      <td>1.749713e-04</td>\n",
              "      <td>0.594025</td>\n",
              "      <td>4.074263e-04</td>\n",
              "      <td>0.671564</td>\n",
              "      <td>0.055509</td>\n",
              "      <td>0.553964</td>\n",
              "      <td>0.326921</td>\n",
              "      <td>0.110933</td>\n",
              "      <td>0.631678</td>\n",
              "      <td>0.452465</td>\n",
              "      <td>0.589341</td>\n",
              "      <td>0.313353</td>\n",
              "      <td>0.007542</td>\n",
              "      <td>0</td>\n",
              "      <td>0.797778</td>\n",
              "      <td>0.002840</td>\n",
              "      <td>0.624156</td>\n",
              "      <td>0.610441</td>\n",
              "      <td>0.840138</td>\n",
              "      <td>0.275789</td>\n",
              "      <td>0.026791</td>\n",
              "      <td>0.565158</td>\n",
              "      <td>0.097649</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6817</th>\n",
              "      <td>0</td>\n",
              "      <td>0.506264</td>\n",
              "      <td>0.559911</td>\n",
              "      <td>0.554045</td>\n",
              "      <td>0.607850</td>\n",
              "      <td>0.607850</td>\n",
              "      <td>0.999074</td>\n",
              "      <td>0.797500</td>\n",
              "      <td>0.809399</td>\n",
              "      <td>0.303498</td>\n",
              "      <td>0.781663</td>\n",
              "      <td>1.236154e-04</td>\n",
              "      <td>2.510000e+09</td>\n",
              "      <td>0.476123</td>\n",
              "      <td>2.110211e-04</td>\n",
              "      <td>0.181294</td>\n",
              "      <td>0.213392</td>\n",
              "      <td>0.213392</td>\n",
              "      <td>0.213392</td>\n",
              "      <td>0.228326</td>\n",
              "      <td>0.346573</td>\n",
              "      <td>0.031535</td>\n",
              "      <td>0.111799</td>\n",
              "      <td>0.185584</td>\n",
              "      <td>0.022350</td>\n",
              "      <td>0.854064</td>\n",
              "      <td>0.696113</td>\n",
              "      <td>0.696113</td>\n",
              "      <td>0.218006</td>\n",
              "      <td>7.250000e+09</td>\n",
              "      <td>0.000529</td>\n",
              "      <td>0.264409</td>\n",
              "      <td>0.401028</td>\n",
              "      <td>0.012782</td>\n",
              "      <td>0.007256</td>\n",
              "      <td>0.630731</td>\n",
              "      <td>0.003816</td>\n",
              "      <td>0.086979</td>\n",
              "      <td>0.913021</td>\n",
              "      <td>0.005529</td>\n",
              "      <td>...</td>\n",
              "      <td>0.353624</td>\n",
              "      <td>0.564439</td>\n",
              "      <td>0.112238</td>\n",
              "      <td>0.007753</td>\n",
              "      <td>7.133218e-03</td>\n",
              "      <td>0.083199</td>\n",
              "      <td>0.380251</td>\n",
              "      <td>0.277353</td>\n",
              "      <td>0.013334</td>\n",
              "      <td>0.893241</td>\n",
              "      <td>0.736713</td>\n",
              "      <td>0.329294</td>\n",
              "      <td>3.200000e+09</td>\n",
              "      <td>0.939613</td>\n",
              "      <td>0.002395</td>\n",
              "      <td>0.016443</td>\n",
              "      <td>2.135940e-04</td>\n",
              "      <td>1.351937e-04</td>\n",
              "      <td>0.593997</td>\n",
              "      <td>1.165392e-04</td>\n",
              "      <td>0.671606</td>\n",
              "      <td>0.246805</td>\n",
              "      <td>0.893241</td>\n",
              "      <td>0.329294</td>\n",
              "      <td>0.110957</td>\n",
              "      <td>0.684857</td>\n",
              "      <td>0.471313</td>\n",
              "      <td>0.678338</td>\n",
              "      <td>0.320118</td>\n",
              "      <td>0.022916</td>\n",
              "      <td>0</td>\n",
              "      <td>0.811808</td>\n",
              "      <td>0.002837</td>\n",
              "      <td>0.623957</td>\n",
              "      <td>0.607846</td>\n",
              "      <td>0.841084</td>\n",
              "      <td>0.277547</td>\n",
              "      <td>0.026822</td>\n",
              "      <td>0.565302</td>\n",
              "      <td>0.044009</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6818</th>\n",
              "      <td>0</td>\n",
              "      <td>0.493053</td>\n",
              "      <td>0.570105</td>\n",
              "      <td>0.549548</td>\n",
              "      <td>0.627409</td>\n",
              "      <td>0.627409</td>\n",
              "      <td>0.998080</td>\n",
              "      <td>0.801987</td>\n",
              "      <td>0.813800</td>\n",
              "      <td>0.313415</td>\n",
              "      <td>0.786079</td>\n",
              "      <td>1.431695e-03</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.427721</td>\n",
              "      <td>5.900000e+08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.220766</td>\n",
              "      <td>0.220766</td>\n",
              "      <td>0.220766</td>\n",
              "      <td>0.227758</td>\n",
              "      <td>0.305793</td>\n",
              "      <td>0.000665</td>\n",
              "      <td>0.092501</td>\n",
              "      <td>0.182119</td>\n",
              "      <td>0.025316</td>\n",
              "      <td>0.848053</td>\n",
              "      <td>0.689527</td>\n",
              "      <td>0.689527</td>\n",
              "      <td>0.217605</td>\n",
              "      <td>9.350000e+09</td>\n",
              "      <td>0.000519</td>\n",
              "      <td>0.264186</td>\n",
              "      <td>0.360102</td>\n",
              "      <td>0.051348</td>\n",
              "      <td>0.040897</td>\n",
              "      <td>0.630618</td>\n",
              "      <td>0.000461</td>\n",
              "      <td>0.014149</td>\n",
              "      <td>0.985851</td>\n",
              "      <td>0.058476</td>\n",
              "      <td>...</td>\n",
              "      <td>0.527136</td>\n",
              "      <td>0.505010</td>\n",
              "      <td>0.238147</td>\n",
              "      <td>0.051481</td>\n",
              "      <td>6.667354e-02</td>\n",
              "      <td>0.018517</td>\n",
              "      <td>0.239585</td>\n",
              "      <td>0.276975</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.737286</td>\n",
              "      <td>0.326690</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.938005</td>\n",
              "      <td>0.002791</td>\n",
              "      <td>0.006089</td>\n",
              "      <td>7.863781e-03</td>\n",
              "      <td>8.238471e-03</td>\n",
              "      <td>0.598674</td>\n",
              "      <td>9.505992e-03</td>\n",
              "      <td>0.672096</td>\n",
              "      <td>0.005016</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.326690</td>\n",
              "      <td>0.110933</td>\n",
              "      <td>0.659917</td>\n",
              "      <td>0.483285</td>\n",
              "      <td>0.505531</td>\n",
              "      <td>0.316238</td>\n",
              "      <td>0.005579</td>\n",
              "      <td>0</td>\n",
              "      <td>0.815956</td>\n",
              "      <td>0.000707</td>\n",
              "      <td>0.626680</td>\n",
              "      <td>0.627408</td>\n",
              "      <td>0.841019</td>\n",
              "      <td>0.275114</td>\n",
              "      <td>0.026793</td>\n",
              "      <td>0.565167</td>\n",
              "      <td>0.233902</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>6819 rows × 95 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      Bankrupt?  ...   Equity to Liability\n",
              "0             1  ...              0.016469\n",
              "1             1  ...              0.020794\n",
              "2             1  ...              0.016474\n",
              "3             1  ...              0.023982\n",
              "4             1  ...              0.035490\n",
              "...         ...  ...                   ...\n",
              "6814          0  ...              0.029890\n",
              "6815          0  ...              0.038284\n",
              "6816          0  ...              0.097649\n",
              "6817          0  ...              0.044009\n",
              "6818          0  ...              0.233902\n",
              "\n",
              "[6819 rows x 95 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oAybaTf_vE8"
      },
      "source": [
        "Το label βρίσκεται στην πρώτη θέση. Επίσης όλα μας τα δεδομένα είναι αριθμητικά, οπότε δεν χρειάζεται να κάνουμε κάποια μετατροπή."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.586665Z",
          "iopub.execute_input": "2021-11-25T16:30:54.587172Z",
          "iopub.status.idle": "2021-11-25T16:30:54.597566Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.587143Z",
          "shell.execute_reply": "2021-11-25T16:30:54.596617Z"
        },
        "trusted": true,
        "id": "x03slj9t-QjK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f181d7a0-6048-4502-91e9-265572f4dd87"
      },
      "source": [
        "rows_count, columns_count = bankruptcy_df.shape\n",
        "print('Total Number of rows :', rows_count)\n",
        "print('Total Number of columns :', columns_count)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Number of rows : 6819\n",
            "Total Number of columns : 96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.599337Z",
          "iopub.execute_input": "2021-11-25T16:30:54.599625Z",
          "iopub.status.idle": "2021-11-25T16:30:54.612209Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.599586Z",
          "shell.execute_reply": "2021-11-25T16:30:54.611563Z"
        },
        "trusted": true,
        "id": "ryrf9ZPj-QjM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec4f768a-f38b-44e8-d2d1-06a06ac2a593"
      },
      "source": [
        "bankruptcy_df.isnull().sum().sum()"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoHaVQWyAOtp"
      },
      "source": [
        "Επίσης δεν έχουμε καμμία κενή τιμή, που μας κάνει την ζωή εύκολη."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.613438Z",
          "iopub.execute_input": "2021-11-25T16:30:54.613782Z",
          "iopub.status.idle": "2021-11-25T16:30:54.621508Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.613755Z",
          "shell.execute_reply": "2021-11-25T16:30:54.620675Z"
        },
        "trusted": true,
        "id": "so91ZUFn-QjO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3942144b-e866-4886-c08d-2252a77a9bd5"
      },
      "source": [
        "bankruptcy_df['Bankrupt?'].value_counts()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    6599\n",
              "1     220\n",
              "Name: Bankrupt?, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wZMDmQwdAUmt"
      },
      "source": [
        "Βλέπουμε ότι το dataset είναι υπερβολικά unbalanced, με τις εταιρείες που χρεοκόπησαν να είναι η μειοψηφία."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.622686Z",
          "iopub.execute_input": "2021-11-25T16:30:54.62325Z",
          "iopub.status.idle": "2021-11-25T16:30:54.631922Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.623194Z",
          "shell.execute_reply": "2021-11-25T16:30:54.631327Z"
        },
        "trusted": true,
        "id": "PpxLzGBw-QjP"
      },
      "source": [
        "X = bankruptcy_df.iloc[:,1:96]\n",
        "y = bankruptcy_df.iloc[:,0]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.633087Z",
          "iopub.execute_input": "2021-11-25T16:30:54.633916Z",
          "iopub.status.idle": "2021-11-25T16:30:54.644969Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.633871Z",
          "shell.execute_reply": "2021-11-25T16:30:54.644269Z"
        },
        "trusted": true,
        "id": "5sEFBJll-QjR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de069344-9e20-43c6-ce82-4a414b27b0f7"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6819, 95)"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.646014Z",
          "iopub.execute_input": "2021-11-25T16:30:54.646482Z",
          "iopub.status.idle": "2021-11-25T16:30:54.656262Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.646448Z",
          "shell.execute_reply": "2021-11-25T16:30:54.655083Z"
        },
        "trusted": true,
        "id": "xxGS1_7q-QjS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5675697a-37b2-4bf9-91f5-1a54ccdc557d"
      },
      "source": [
        "y.shape"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6819,)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9p6TzZwM4SG"
      },
      "source": [
        "#Out of box classifiers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "342tHhkuBddp"
      },
      "source": [
        "Θα τρέξουμε τους ταξινομητές μας για διάφορα train test split για να δούμε ποιό εξυπηρετεί καλύτερα το dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.657185Z",
          "iopub.execute_input": "2021-11-25T16:30:54.657848Z",
          "iopub.status.idle": "2021-11-25T16:30:54.666452Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.657816Z",
          "shell.execute_reply": "2021-11-25T16:30:54.665632Z"
        },
        "trusted": true,
        "id": "kKZQjswZ-Qjc"
      },
      "source": [
        "out_of_the_box_data = {}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAt3BM_vBrK-"
      },
      "source": [
        "##MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:30:54.668112Z",
          "iopub.execute_input": "2021-11-25T16:30:54.668684Z",
          "iopub.status.idle": "2021-11-25T16:31:27.49427Z",
          "shell.execute_reply.started": "2021-11-25T16:30:54.668643Z",
          "shell.execute_reply": "2021-11-25T16:31:27.493443Z"
        },
        "trusted": true,
        "id": "LPryWCAR-Qje",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b21481b6-6153-4a96-95f6-293182661b3d"
      },
      "source": [
        "test_splits = [0.15,0.2,0.25,0.3,0.4]\n",
        "\n",
        "for test_split in test_splits:\n",
        "    name = \"MLP_\"+ str(test_split)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = test_split,random_state = 10)\n",
        "    clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy =accuracy_score(y_test,predictions)\n",
        "    recall = recall_score(y_test,predictions)\n",
        "    f1 = f1_score(y_test,predictions,average='macro')\n",
        "    accuracy_score_balanced = balanced_accuracy_score(y_test,predictions)\n",
        "    out_of_the_box_data[name] = (predictions,accuracy,recall,f1,accuracy_score_balanced)\n",
        "    print(\"For test size \",test_split, \" Accuracy : \",accuracy)\n",
        "    print(\"For test size \",test_split, \" Recall :   \",recall)\n",
        "    print(\"For test size \",test_split, \" F1 :       \",f1)\n",
        "    print(\"For test size \",test_split, \" balanced_accuracy_score :       \",accuracy_score_balanced)\n"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For test size  0.15  Accuracy :  0.9550342130987293\n",
            "For test size  0.15  Recall :    0.02631578947368421\n",
            "For test size  0.15  F1 :        0.509321821821822\n",
            "For test size  0.15  balanced_accuracy_score :        0.5085893668180604\n",
            "For test size  0.2  Accuracy :  0.9413489736070382\n",
            "For test size  0.2  Recall :    0.09803921568627451\n",
            "For test size  0.2  F1 :        0.5403925532811052\n",
            "For test size  0.2  balanced_accuracy_score :        0.5360721592521244\n",
            "For test size  0.25  Accuracy :  0.9249266862170088\n",
            "For test size  0.25  Recall :    0.05\n",
            "For test size  0.25  F1 :        0.502852040165473\n",
            "For test size  0.25  balanced_accuracy_score :        0.503419452887538\n",
            "For test size  0.3  Accuracy :  0.9389051808406648\n",
            "For test size  0.3  Recall :    0.0684931506849315\n",
            "For test size  0.3  F1 :        0.5212422430011513\n",
            "For test size  0.3  balanced_accuracy_score :        0.5198015677398302\n",
            "For test size  0.4  Accuracy :  0.9149560117302052\n",
            "For test size  0.4  Recall :    0.16129032258064516\n",
            "For test size  0.4  F1 :        0.5349184467103851\n",
            "For test size  0.4  balanced_accuracy_score :        0.5514231499051233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJmkcMpPBtVJ"
      },
      "source": [
        "##SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:31:27.49574Z",
          "iopub.execute_input": "2021-11-25T16:31:27.496664Z",
          "iopub.status.idle": "2021-11-25T16:31:32.546715Z",
          "shell.execute_reply.started": "2021-11-25T16:31:27.496613Z",
          "shell.execute_reply": "2021-11-25T16:31:32.545882Z"
        },
        "trusted": true,
        "id": "NCEyE91q-Qjf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54f98a77-6b5c-40f3-ce03-db5aff72bc61"
      },
      "source": [
        "for test_split in test_splits:\n",
        "    name = \"SVC_\"+ str(test_split)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = test_split,random_state = 10)\n",
        "    clf = SVC().fit(X_train, y_train)\n",
        "    predictions = clf.predict(X_test)\n",
        "    accuracy =accuracy_score(y_test,predictions)\n",
        "    recall = recall_score(y_test,predictions)\n",
        "    f1 = f1_score(y_test,predictions,average='macro')\n",
        "    accuracy_score_balanced =balanced_accuracy_score(y_test,predictions)\n",
        "    out_of_the_box_data[name] = (predictions,accuracy,recall,f1,accuracy_score_balanced)\n",
        "    print(\"For test size \",test_split, \" Accuracy : \",accuracy)\n",
        "    print(\"For test size \",test_split, \" Recall :   \",recall)\n",
        "    print(\"For test size \",test_split, \" F1 :       \",f1)\n",
        "    print(\"For test size \",test_split, \" balanced_accuracy_score :       \",accuracy_score_balanced)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For test size  0.15  Accuracy :  0.9628543499511242\n",
            "For test size  0.15  Recall :    0.0\n",
            "For test size  0.15  F1 :        0.4905378486055777\n",
            "For test size  0.15  balanced_accuracy_score :        0.5\n",
            "For test size  0.2  Accuracy :  0.9626099706744868\n",
            "For test size  0.2  Recall :    0.0\n",
            "For test size  0.2  F1 :        0.4904744116548375\n",
            "For test size  0.2  balanced_accuracy_score :        0.5\n",
            "For test size  0.25  Accuracy :  0.9648093841642229\n",
            "For test size  0.25  Recall :    0.0\n",
            "For test size  0.25  F1 :        0.491044776119403\n",
            "For test size  0.25  balanced_accuracy_score :        0.5\n",
            "For test size  0.3  Accuracy :  0.9643206256109482\n",
            "For test size  0.3  Recall :    0.0\n",
            "For test size  0.3  F1 :        0.4909181388405076\n",
            "For test size  0.3  balanced_accuracy_score :        0.5\n",
            "For test size  0.4  Accuracy :  0.9659090909090909\n",
            "For test size  0.4  Recall :    0.0\n",
            "For test size  0.4  F1 :        0.49132947976878616\n",
            "For test size  0.4  balanced_accuracy_score :        0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhkbuT0sBqST"
      },
      "source": [
        "##BAR PLOTS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zEVKHBDFBw_K"
      },
      "source": [
        "###BAR PLOT FUNCTION DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:35:21.951547Z",
          "iopub.execute_input": "2021-11-25T16:35:21.952066Z",
          "iopub.status.idle": "2021-11-25T16:35:21.959794Z",
          "shell.execute_reply.started": "2021-11-25T16:35:21.95203Z",
          "shell.execute_reply": "2021-11-25T16:35:21.959057Z"
        },
        "trusted": true,
        "id": "yay4XjSw-Qjg"
      },
      "source": [
        "def BarPlots(keys,accuracies,recalls,F1s,balanced_accs):\n",
        "  fig, (ax1, ax2,ax3,ax4) = plt.subplots(1,4,figsize=(18,6))\n",
        "\n",
        "\n",
        "  ax1.tick_params(axis='x', rotation=45)\n",
        "  ax1.set_title('Accuracies')\n",
        "  ax1.bar(keys, accuracies)\n",
        "    \n",
        "    \n",
        "  ax2.tick_params(axis='x', rotation=45)\n",
        "  ax2.set_title('Recall')\n",
        "  ax2.bar(keys, recalls)\n",
        "\n",
        "  ax3.tick_params(axis='x', rotation=45)\n",
        "  ax3.set_title('F1 Macro')\n",
        "  ax3.bar(keys, F1s)\n",
        "    \n",
        "  ax4.tick_params(axis='x', rotation=45)\n",
        "  ax4.set_title('balanced_accuracy_score')\n",
        "  ax4.bar(keys, balanced_accs)\n",
        "\n"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:35:25.445773Z",
          "iopub.execute_input": "2021-11-25T16:35:25.446073Z",
          "iopub.status.idle": "2021-11-25T16:35:26.151025Z",
          "shell.execute_reply.started": "2021-11-25T16:35:25.446041Z",
          "shell.execute_reply": "2021-11-25T16:35:26.150223Z"
        },
        "trusted": true,
        "id": "Ha3FjbVN-Qjh"
      },
      "source": [
        "out_of_the_box_accuracies =[]\n",
        "out_of_the_box_F1s =[]\n",
        "out_of_the_box_recalls = []\n",
        "out_of_the_box_recalls_balanced_acs =[]\n",
        "for key in out_of_the_box_data:\n",
        "    out_of_the_box_accuracies.append(out_of_the_box_data[key][1])\n",
        "    out_of_the_box_recalls.append(out_of_the_box_data[key][2])\n",
        "    out_of_the_box_F1s.append(out_of_the_box_data[key][3])\n",
        "    out_of_the_box_recalls_balanced_acs.append(out_of_the_box_data[key][4])\n",
        "\n",
        "    \n"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        },
        "id": "boaFysXhtB2J",
        "outputId": "730853e4-ecf9-46f7-d216-dbab99b948f9"
      },
      "source": [
        "BarPlots(out_of_the_box_data.keys(),out_of_the_box_accuracies,out_of_the_box_recalls,out_of_the_box_F1s,out_of_the_box_recalls_balanced_acs)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1296x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBXM6KQADSns"
      },
      "source": [
        "##MARK DOWN TABLE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rpdzHWKDVRe"
      },
      "source": [
        "###MARK DOWN FUNCTION DEFINITION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:20:55.205953Z",
          "iopub.execute_input": "2021-11-25T16:20:55.206377Z",
          "iopub.status.idle": "2021-11-25T16:20:55.216699Z",
          "shell.execute_reply.started": "2021-11-25T16:20:55.206339Z",
          "shell.execute_reply": "2021-11-25T16:20:55.216017Z"
        },
        "trusted": true,
        "id": "wBDVubJl-Qji"
      },
      "source": [
        "def markdownGenerator():\n",
        "  names = ['Accuracies','Recall','F1','Balanced Accuracy Score']\n",
        "  header = '| |'\n",
        "  for name in names:\n",
        "    header = header+\" \" +name+' |'\n",
        "\n",
        "  print(header)\n",
        "\n",
        "  split = '|--|'\n",
        "  for _ in names:\n",
        "    split = split+ '--|'\n",
        "\n",
        "  print(split)\n",
        "  keys = out_of_the_box_data.keys()\n",
        "  for key in keys:\n",
        "    line = '|'+key+'|'\n",
        "    for i in range(1,5):\n",
        "        line = line+ str(out_of_the_box_data[key][i]) + '|'\n",
        "    print(line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHsxQqg1tVPV",
        "outputId": "33b2bfbd-89df-47d7-a2ab-23fe203031df"
      },
      "source": [
        "markdownGenerator()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| | Accuracies | Recall | F1 | Balanced Accuracy Score |\n",
            "|--|--|--|--|--|\n",
            "|MLP_0.15|0.9550342130987293|0.02631578947368421|0.509321821821822|0.5085893668180604|\n",
            "|MLP_0.2|0.9413489736070382|0.09803921568627451|0.5403925532811052|0.5360721592521244|\n",
            "|MLP_0.25|0.9249266862170088|0.05|0.502852040165473|0.503419452887538|\n",
            "|MLP_0.3|0.9389051808406648|0.0684931506849315|0.5212422430011513|0.5198015677398302|\n",
            "|MLP_0.4|0.9149560117302052|0.16129032258064516|0.5349184467103851|0.5514231499051233|\n",
            "|SVC_0.15|0.9628543499511242|0.0|0.4905378486055777|0.5|\n",
            "|SVC_0.2|0.9626099706744868|0.0|0.4904744116548375|0.5|\n",
            "|SVC_0.25|0.9648093841642229|0.0|0.491044776119403|0.5|\n",
            "|SVC_0.3|0.9643206256109482|0.0|0.4909181388405076|0.5|\n",
            "|SVC_0.4|0.9659090909090909|0.0|0.49132947976878616|0.5|\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLWnTbzb-Qjj"
      },
      "source": [
        "| | Accuracies | Recall | F1 | Balanced Accuracy Score |\n",
        "|--|--|--|--|--|\n",
        "|MLP_0.15|0.9550342130987293|0.02631578947368421|0.509321821821822|0.5085893668180604|\n",
        "|MLP_0.2|0.9413489736070382|0.09803921568627451|0.5403925532811052|0.5360721592521244|\n",
        "|MLP_0.25|0.9249266862170088|0.05|0.502852040165473|0.503419452887538|\n",
        "|MLP_0.3|0.9389051808406648|0.0684931506849315|0.5212422430011513|0.5198015677398302|\n",
        "|MLP_0.4|0.9149560117302052|0.16129032258064516|0.5349184467103851|0.5514231499051233|\n",
        "|SVC_0.15|0.9628543499511242|0.0|0.4905378486055777|0.5|\n",
        "|SVC_0.2|0.9626099706744868|0.0|0.4904744116548375|0.5|\n",
        "|SVC_0.25|0.9648093841642229|0.0|0.491044776119403|0.5|\n",
        "|SVC_0.3|0.9643206256109482|0.0|0.4909181388405076|0.5|\n",
        "|SVC_0.4|0.9659090909090909|0.0|0.49132947976878616|0.5|\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fz23433l-Qjk"
      },
      "source": [
        "Τα συνεχή μηδενικά στον SVC είναι λίγο ύποπτα, οπότε κάνουμε έναν έλεγχο."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:20:55.218221Z",
          "iopub.execute_input": "2021-11-25T16:20:55.218664Z",
          "iopub.status.idle": "2021-11-25T16:20:55.22612Z",
          "shell.execute_reply.started": "2021-11-25T16:20:55.218625Z",
          "shell.execute_reply": "2021-11-25T16:20:55.225354Z"
        },
        "trusted": true,
        "id": "SrE5ODCl-Qjk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e20be500-8f58-48ff-f3fd-06e45d5175f2"
      },
      "source": [
        "len(out_of_the_box_data['SVC_0.15'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1023"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:20:55.227416Z",
          "iopub.execute_input": "2021-11-25T16:20:55.227614Z",
          "iopub.status.idle": "2021-11-25T16:20:55.235505Z",
          "shell.execute_reply.started": "2021-11-25T16:20:55.227585Z",
          "shell.execute_reply": "2021-11-25T16:20:55.234594Z"
        },
        "trusted": true,
        "id": "Qz1EsQsO-Qjn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "906ffbe7-c363-4686-8a71-0cdcd7e0ddee"
      },
      "source": [
        "np.count_nonzero(out_of_the_box_data['SVC_0.15'][0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GDgb0DoDleb"
      },
      "source": [
        "##Συμπεράσματα"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnDxVDJe-Qjo"
      },
      "source": [
        "Βλέπουμε ότι καθώς το dataset είναι πάρα πολύ unbalanced, και οι δύο Classifiers, αλλα κυρίως ο SVC αναγνωρίζει ότι εάν τείνει στον dummy constant 0 θα βρεί αρκετά καλό accuracy. Για αυτό τον λόγο θα δουλέψουμε με recall , F1 και balanced accuracy score. Από τις παραπάνω δοκιμές βλέπουμε ότι η καλύτερη τιμή του test score για τις μετρικές που μας ενδιαφέρουν είναι 60% - 40 %.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDvDfgntPJiO"
      },
      "source": [
        "#Data process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woDpsTBOXtDR"
      },
      "source": [
        "##Correlation Threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kwCivwa3m9ge"
      },
      "source": [
        "Με μία πρώτη ανάγνωση του dataset βλέπουμε ότι υπάρχουν αρκετές παράμετροι έχουν ισχυρό συσχετισμό μεταξύ τους. Για αυτό τον λόγο θεωρήσαμε ότι θα ήταν καλό να κάνουμε drop τις στήλες που ευθύνονται για μικρό ποσοστό του variance. Για αυτόν τον λόγο θα χρησιμοποιήσουμε μία συνάρτηση από το stack overflow που κάνει αυτή την δουλειά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:20:55.236846Z",
          "iopub.execute_input": "2021-11-25T16:20:55.237186Z",
          "iopub.status.idle": "2021-11-25T16:20:55.247377Z",
          "shell.execute_reply.started": "2021-11-25T16:20:55.237148Z",
          "shell.execute_reply": "2021-11-25T16:20:55.246475Z"
        },
        "trusted": true,
        "id": "xuVE5Ts3-Qjp"
      },
      "source": [
        "#https://stackoverflow.com/a/51658973/11333604\n",
        "\n",
        "def corr_df(x, corr_val=0.6):\n",
        "    '''\n",
        "    Obj: Drops features that are strongly correlated to other features.\n",
        "          This lowers model complexity, and aids in generalizing the model.\n",
        "    Inputs:\n",
        "          df: features df (x)\n",
        "          corr_val: Columns are dropped relative to the corr_val input (e.g. 0.8)\n",
        "    Output: df that only includes uncorrelated features\n",
        "    '''\n",
        "\n",
        "    # Creates Correlation Matrix and Instantiates\n",
        "    corr_matrix = x.corr()\n",
        "    iters = range(len(corr_matrix.columns) - 1)\n",
        "    drop_cols = []\n",
        "\n",
        "    # Iterates through Correlation Matrix Table to find correlated columns\n",
        "    for i in iters:\n",
        "        for j in range(i):\n",
        "            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n",
        "            col = item.columns\n",
        "            row = item.index\n",
        "            val = item.values\n",
        "            if abs(val) >= corr_val:\n",
        "                # Prints the correlated feature set and the corr val\n",
        "                print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n",
        "                drop_cols.append(i)\n",
        "\n",
        "    drops = sorted(set(drop_cols))[::-1]\n",
        "\n",
        "    # Drops the correlated columns\n",
        "    for i in drops:\n",
        "        col = x.iloc[:, (i+1):(i+2)].columns.values\n",
        "        x = x.drop(col, axis=1)\n",
        "    return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AVCoFTNOzcD",
        "outputId": "fc792859-e8da-4957-887f-2b399a7c20e0"
      },
      "source": [
        "X = corr_df(X)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ROA(B) before interest and depreciation after tax |  ROA(C) before interest and depreciation before interest | 0.99\n",
            " After-tax net Interest Rate |  Operating Profit Rate | 0.86\n",
            " Continuous interest rate (after tax) |  Operating Profit Rate | 0.92\n",
            " Continuous interest rate (after tax) |  Pre-tax net Interest Rate | 0.99\n",
            " Continuous interest rate (after tax) |  After-tax net Interest Rate | 0.98\n",
            " Net Value Per Share (C) |  Net Value Per Share (B) | 1.0\n",
            " Persistent EPS in the Last Four Seasons |  ROA(C) before interest and depreciation before interest | 0.78\n",
            " Persistent EPS in the Last Four Seasons |  ROA(A) before interest and % after tax | 0.76\n",
            " Persistent EPS in the Last Four Seasons |  ROA(B) before interest and depreciation after tax | 0.76\n",
            " Persistent EPS in the Last Four Seasons |  Net Value Per Share (B) | 0.76\n",
            " Persistent EPS in the Last Four Seasons |  Net Value Per Share (A) | 0.76\n",
            " Operating Profit Per Share (Yuan ¥) |  ROA(C) before interest and depreciation before interest | 0.69\n",
            " Operating Profit Per Share (Yuan ¥) |  ROA(A) before interest and % after tax | 0.65\n",
            " Operating Profit Per Share (Yuan ¥) |  ROA(B) before interest and depreciation after tax | 0.66\n",
            " Operating Profit Per Share (Yuan ¥) |  Net Value Per Share (B) | 0.61\n",
            " Operating Profit Per Share (Yuan ¥) |  Net Value Per Share (A) | 0.61\n",
            " Operating Profit Per Share (Yuan ¥) |  Net Value Per Share (C) | 0.61\n",
            " Operating Profit Per Share (Yuan ¥) |  Persistent EPS in the Last Four Seasons | 0.88\n",
            " Per Share Net profit before tax (Yuan ¥) |  ROA(C) before interest and depreciation before interest | 0.75\n",
            " Per Share Net profit before tax (Yuan ¥) |  ROA(A) before interest and % after tax | 0.75\n",
            " Per Share Net profit before tax (Yuan ¥) |  ROA(B) before interest and depreciation after tax | 0.72\n",
            " Per Share Net profit before tax (Yuan ¥) |  Net Value Per Share (B) | 0.73\n",
            " Per Share Net profit before tax (Yuan ¥) |  Net Value Per Share (A) | 0.73\n",
            " Per Share Net profit before tax (Yuan ¥) |  Net Value Per Share (C) | 0.73\n",
            " Per Share Net profit before tax (Yuan ¥) |  Persistent EPS in the Last Four Seasons | 0.96\n",
            " Regular Net Profit Growth Rate |  Operating Profit Growth Rate | 0.64\n",
            " Cash Reinvestment % |  Cash Flow Per Share | 0.65\n",
            " Operating profit/Paid-in capital |  ROA(C) before interest and depreciation before interest | 0.69\n",
            " Operating profit/Paid-in capital |  ROA(A) before interest and % after tax | 0.65\n",
            " Operating profit/Paid-in capital |  ROA(B) before interest and depreciation after tax | 0.66\n",
            " Operating profit/Paid-in capital |  Net Value Per Share (B) | 0.6\n",
            " Operating profit/Paid-in capital |  Net Value Per Share (A) | 0.6\n",
            " Operating profit/Paid-in capital |  Net Value Per Share (C) | 0.6\n",
            " Operating profit/Paid-in capital |  Persistent EPS in the Last Four Seasons | 0.87\n",
            " Operating profit/Paid-in capital |  Operating Profit Per Share (Yuan ¥) | 1.0\n",
            " Operating profit/Paid-in capital |  Per Share Net profit before tax (Yuan ¥) | 0.86\n",
            " Net profit before tax/Paid-in capital |  ROA(C) before interest and depreciation before interest | 0.75\n",
            " Net profit before tax/Paid-in capital |  ROA(A) before interest and % after tax | 0.76\n",
            " Net profit before tax/Paid-in capital |  ROA(B) before interest and depreciation after tax | 0.73\n",
            " Net profit before tax/Paid-in capital |  Net Value Per Share (B) | 0.71\n",
            " Net profit before tax/Paid-in capital |  Net Value Per Share (A) | 0.71\n",
            " Net profit before tax/Paid-in capital |  Net Value Per Share (C) | 0.71\n",
            " Net profit before tax/Paid-in capital |  Persistent EPS in the Last Four Seasons | 0.96\n",
            " Net profit before tax/Paid-in capital |  Operating Profit Per Share (Yuan ¥) | 0.89\n",
            " Net profit before tax/Paid-in capital |  Per Share Net profit before tax (Yuan ¥) | 0.96\n",
            " Inventory and accounts receivable/Net value |  Borrowing dependency | 0.7\n",
            " Net Worth Turnover Rate (times) |  Total Asset Turnover | 0.76\n",
            " Current Assets/Total Assets |  Working Capital to Total Assets | 0.71\n",
            " Current Liability to Assets |  Debt ratio % | 0.84\n",
            " Current Liability to Assets |  Net worth/Assets | -0.84\n",
            " Operating Funds to Liability |  Cash flow rate | 0.88\n",
            " Working Capital/Equity |  Contingent liabilities/Net worth | -0.77\n",
            " Current Liabilities/Equity |  Borrowing dependency | 0.89\n",
            " Current Liabilities/Equity |  Contingent liabilities/Net worth | 0.62\n",
            " Current Liabilities/Equity |  Inventory and accounts receivable/Net value | 0.66\n",
            " Retained Earnings to Total Assets |  ROA(C) before interest and depreciation before interest | 0.65\n",
            " Retained Earnings to Total Assets |  ROA(A) before interest and % after tax | 0.72\n",
            " Retained Earnings to Total Assets |  ROA(B) before interest and depreciation after tax | 0.67\n",
            " Working capitcal Turnover Rate |  Non-industry income and expenditure/revenue | 0.74\n",
            " Cash Flow to Sales |  Non-industry income and expenditure/revenue | 0.68\n",
            " Cash Flow to Sales |  Working capitcal Turnover Rate | 0.95\n",
            " Current Liability to Liability |  Current Liabilities/Liability | 1.0\n",
            " Current Liability to Equity |  Borrowing dependency | 0.89\n",
            " Current Liability to Equity |  Contingent liabilities/Net worth | 0.62\n",
            " Current Liability to Equity |  Inventory and accounts receivable/Net value | 0.66\n",
            " Current Liability to Equity |  Working Capital/Equity | -0.69\n",
            " Current Liability to Equity |  Current Liabilities/Equity | 1.0\n",
            " Equity to Long-term Liability |  Borrowing dependency | 0.81\n",
            " CFO to Assets |  Cash flow rate | 0.6\n",
            " CFO to Assets |  Cash Flow Per Share | 0.72\n",
            " CFO to Assets |  Cash Reinvestment % | 0.74\n",
            " CFO to Assets |  Operating Funds to Liability | 0.7\n",
            " Current Liability to Current Assets |  Working Capital to Total Assets | -0.63\n",
            " Net Income to Total Assets |  ROA(C) before interest and depreciation before interest | 0.89\n",
            " Net Income to Total Assets |  ROA(A) before interest and % after tax | 0.96\n",
            " Net Income to Total Assets |  ROA(B) before interest and depreciation after tax | 0.91\n",
            " Net Income to Total Assets |  Persistent EPS in the Last Four Seasons | 0.69\n",
            " Net Income to Total Assets |  Per Share Net profit before tax (Yuan ¥) | 0.67\n",
            " Net Income to Total Assets |  Net profit before tax/Paid-in capital | 0.68\n",
            " Net Income to Total Assets |  Retained Earnings to Total Assets | 0.79\n",
            " Gross Profit to Sales |  Operating Gross Margin | 1.0\n",
            " Gross Profit to Sales |  Realized Sales Gross Margin | 1.0\n",
            " Net Income to Stockholder's Equity |  Borrowing dependency | -0.81\n",
            " Net Income to Stockholder's Equity |  Current Liabilities/Equity | -0.75\n",
            " Net Income to Stockholder's Equity |  Current Liability to Equity | -0.75\n",
            " Net Income to Stockholder's Equity |  Equity to Long-term Liability | -0.62\n",
            " Liability to Equity |  Borrowing dependency | 0.96\n",
            " Liability to Equity |  Contingent liabilities/Net worth | 0.62\n",
            " Liability to Equity |  Inventory and accounts receivable/Net value | 0.67\n",
            " Liability to Equity |  Working Capital/Equity | -0.65\n",
            " Liability to Equity |  Current Liabilities/Equity | 0.96\n",
            " Liability to Equity |  Current Liability to Equity | 0.96\n",
            " Liability to Equity |  Equity to Long-term Liability | 0.78\n",
            " Equity to Liability |  Debt ratio % | -0.63\n",
            " Equity to Liability |  Net worth/Assets | 0.63\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfiWWMCAO26I",
        "outputId": "d772f6fa-61e7-42a8-bc3b-4dbabe056191"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6819, 64)"
            ]
          },
          "metadata": {},
          "execution_count": 106
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KxK7EaCFoDOA"
      },
      "source": [
        "Βλέπουμε ότι κόβει περίπου 30 στοιχεία. το οποία λογικά θα κάνει την εκπαίδευση αρκετά πιο γρήγορη."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bq3KLdygWp9V"
      },
      "source": [
        "##Variance Threshold"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6ca0Es-WqrD"
      },
      "source": [
        "selector = VarianceThreshold()\n",
        "X = selector.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIfYwL0NWwE8",
        "outputId": "ac1abd5f-2fa7-4cb9-8231-3fa226770379"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6819, 63)"
            ]
          },
          "metadata": {},
          "execution_count": 108
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EM-usurAoOPi"
      },
      "source": [
        "Το variance threshold κόβει μόνο μία στήλη."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KklsaJBQTifb"
      },
      "source": [
        "#Smote\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEpZ-6uN-Qju"
      },
      "source": [
        "Το κυριότερο πρόβλημα του dataset είναι ότι είναι ακραία unbalanced. Για αυτό, όπως βλέπουμε και στο hottest kaggle notebook για αυτό το dataset, θα κάνουμε oversampling στην κλάση 1, για να έχουμε ίσο αριθμό instances. \n",
        "Θα τον SVC δοκιμάσουμε λοιπόν στα νέα δεδομένα, μιάς και ήταν το worst case scenario στην προηγούμενη περίπτωση. Test size θα βάλουμε 0.4, στο οποία είχαμε την καλύτεη συμπεριφορά του MLP."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.status.busy": "2021-11-25T16:20:55.782709Z",
          "iopub.execute_input": "2021-11-25T16:20:55.783419Z",
          "iopub.status.idle": "2021-11-25T16:20:55.834462Z",
          "shell.execute_reply.started": "2021-11-25T16:20:55.783371Z",
          "shell.execute_reply": "2021-11-25T16:20:55.833669Z"
        },
        "trusted": true,
        "id": "SAoz9WeK-Qjw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7d4378d-34c3-448a-f483-da8466e21b13"
      },
      "source": [
        "sm = SMOTE(random_state=42)\n",
        "\n",
        "X_sm, y_sm = sm.fit_resample(X, y)\n",
        "\n",
        "print('New balance of 1 and 0 classes (%):')\n",
        "y_sm.value_counts()\n",
        "\n",
        "# https://www.kaggle.com/petrkolar/ml-workflow-0-99-f1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New balance of 1 and 0 classes (%):\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    6599\n",
              "0    6599\n",
              "Name: Bankrupt?, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NZhnMayJkjK"
      },
      "source": [
        "####Smote SVC test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wgaFpbd-Xfu-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4413388-4433-419f-ebab-b0884dabe716"
      },
      "source": [
        "test_split = 0.4\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_sm,y_sm,test_size = test_split,random_state = 10)\n",
        "\n",
        "clf= SVC().fit(X_train, y_train)\n",
        "predictions = clf.predict(X_test)\n",
        "accuracy =accuracy_score(y_test,predictions)\n",
        "recall = recall_score(y_test,predictions)\n",
        "f1 = f1_score(y_test,predictions,average='macro')\n",
        "accuracy_score_balanced =balanced_accuracy_score(y_test,predictions)\n",
        "print(\"For test size \",test_split, \" Accuracy : \",accuracy)\n",
        "print(\"For test size \",test_split, \" Recall :   \",recall)\n",
        "print(\"For test size \",test_split, \" F1 :       \",f1)\n",
        "print(\"For test size \",test_split, \" balanced_accuracy_score :       \",accuracy_score_balanced)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For test size  0.4  Accuracy :  0.8185606060606061\n",
            "For test size  0.4  Recall :    0.871264367816092\n",
            "For test size  0.4  F1 :        0.8182275809201057\n",
            "For test size  0.4  balanced_accuracy_score :        0.8191527831589823\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZieVqzX-Qjz"
      },
      "source": [
        "Βλέπουμε φοβερή βελτίωση ως προς την συμπερφορά του ως προς τις προβλέψεις 0. Τώρα θα ψάξουμε και για τις υπόλοιπες υπερπαραμέτους. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZcaSIXFJnMl"
      },
      "source": [
        "#Optuna GridSearch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AhY8cl2P-Qj0"
      },
      "source": [
        "Θα χρησιμοποιήσουμε το optuna για να βρούμε τις καλύτερες υπερπαραμέτρους."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyNcLoup-Qj1"
      },
      "source": [
        "def objective(trial):\n",
        "  #PREPROCESSING\n",
        "  pipelineList =[]\n",
        "  StandardScalerChoice = trial.suggest_int('StandardScaler',0,1)\n",
        "  if StandardScalerChoice:\n",
        "          pipelineList.append(['scaler', StandardScaler()])\n",
        "\n",
        "\n",
        "  test_splits = trial.suggest_discrete_uniform('test size',0.1,0.4,0.1)\n",
        "  x_train,x_test,y_train,y_test = train_test_split(X_sm,y_sm,test_size=test_splits)\n",
        "\n",
        "\n",
        "  classifier_name = trial.suggest_categorical(\"classifier\", [\"SVC\",\"MLP\"])\n",
        "\n",
        "\n",
        "  if classifier_name == \"SVC\":\n",
        "    C = trial.suggest_loguniform('C', 1e-10, 1)\n",
        "    kernel = trial.suggest_categorical('kernel',['rbf','sigmoid'])\n",
        "    gamma = trial.suggest_categorical('gamma',['auto','scale'])\n",
        "    \n",
        "    pipelineList.append(['svc',SVC(C=C, kernel=kernel,gamma=gamma)])\n",
        "\n",
        "\n",
        "\n",
        "  if classifier_name == \"MLP\":\n",
        "\n",
        "    alpha = trial.suggest_loguniform('alpha',1e-13,1e-8)\n",
        "    solver = trial.suggest_categorical('solver',['sgd','adam'])\n",
        "    activation = trial.suggest_categorical('activation',['logistic','tanh','relu'])\n",
        "    learning_rate = trial.suggest_categorical('learning_rate',['constant','adaptive'])\n",
        "\n",
        "    pipelineList.append(['mlp',MLPClassifier(alpha=alpha, solver=solver, activation=activation,learning_rate=learning_rate)])\n",
        "\n",
        "  clf =Pipeline(steps = pipelineList) \n",
        "\n",
        "  clf.fit(x_train,y_train)\n",
        "\n",
        "  # Evaluate the model\n",
        "  y_pred_test = clf.predict(x_test)\n",
        "  accuracy =accuracy_score(y_test,y_pred_test)\n",
        "  f1 = f1_score(y_test,y_pred_test,average='macro')\n",
        "  return f1, accuracy\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiYv1Wmeolla"
      },
      "source": [
        "Θα σώσουμε τα trials σε ένα pickle αρχείο, όχι ως μέρος της άσκσησς αλλά για να μπορέσουμε να το αφήσουμε και να αναλύσουμε τα δεδομένα άλλη φορά."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cwO0ybcJldk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9dbdcd8-6f13-4d1e-ac0f-30b1886ba1af"
      },
      "source": [
        "study = optuna.create_study(directions=['maximize','maximize'])\n",
        "\n",
        "study.optimize(objective, n_trials=200)\n",
        "\n",
        "\n",
        "studyFileName = 'drive/MyDrive/NEURAL/first/kaggle/study' \n",
        "outfile = open(studyFileName,'wb')\n",
        "pickle.dump(study.get_trials(),outfile)\n",
        "outfile.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m[I 2021-12-02 18:40:45,948]\u001b[0m A new study created in memory with name: no-name-f5e230c1-7186-495d-a773-e6c61f2f9b1a\u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:40:54,321]\u001b[0m Trial 0 finished with values: [0.8234831421962766, 0.8265151515151515] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 2.4029040031714865e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:41:18,599]\u001b[0m Trial 1 finished with values: [0.6539663486413212, 0.6545454545454545] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.5391091063127621e-12, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:41:20,664]\u001b[0m Trial 2 finished with values: [0.5920433473313396, 0.5920454545454545] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:41:22,608]\u001b[0m Trial 3 finished with values: [0.5916893582572857, 0.5924242424242424] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 4.946263295005424e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:41:33,545]\u001b[0m Trial 4 finished with values: [0.31712364200724263, 0.4643939393939394] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.01192107798646013, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:41:45,081]\u001b[0m Trial 5 finished with values: [0.33198380566801616, 0.49696969696969695] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 4.0734332919428933e-10, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:42:03,990]\u001b[0m Trial 6 finished with values: [0.33198380566801616, 0.49696969696969695] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 5.5586302505358384e-09, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:42:17,114]\u001b[0m Trial 7 finished with values: [0.3299492385786802, 0.49242424242424243] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'SVC', 'C': 2.2843091500720636e-09, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:42:17,768]\u001b[0m Trial 8 finished with values: [0.3469677685541632, 0.5145833333333333] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 4.738979631252411e-09, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:42:28,132]\u001b[0m Trial 9 finished with values: [0.33324914761964897, 0.49981060606060607] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.0006750752341722539, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:42:42,559]\u001b[0m Trial 10 finished with values: [0.9838346727898967, 0.9838383838383838] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:43:00,831]\u001b[0m Trial 11 finished with values: [0.32755985736118187, 0.4871212121212121] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 3.4854608825260474e-07, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:43:19,594]\u001b[0m Trial 12 finished with values: [0.6585733176608876, 0.6585858585858586] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 7.3814401753163066e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:43:21,069]\u001b[0m Trial 13 finished with values: [0.48492063492063486, 0.48598484848484846] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 2.45255492903631e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:43:35,598]\u001b[0m Trial 14 finished with values: [0.32342388518708354, 0.478030303030303] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 4.0327625186680925e-08, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:43:46,582]\u001b[0m Trial 15 finished with values: [0.33198380566801616, 0.49696969696969695] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 4.021634422357695e-10, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:44:02,777]\u001b[0m Trial 16 finished with values: [0.3324905183312263, 0.4981060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 2.4124175917029876e-07, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:44:20,373]\u001b[0m Trial 17 finished with values: [0.8498697296196991, 0.8522727272727273] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.00031905157474440256, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:44:34,021]\u001b[0m Trial 18 finished with values: [0.33187109836342166, 0.4967171717171717] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 1.3544219733466798e-09, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:44:44,961]\u001b[0m Trial 19 finished with values: [0.3285859613428281, 0.4893939393939394] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0030870460682219025, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:45:05,586]\u001b[0m Trial 20 finished with values: [0.9203128255018891, 0.9204545454545454] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 2.5937943078584336e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:45:25,489]\u001b[0m Trial 21 finished with values: [0.9779591484216436, 0.978030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.6513562776140843e-11, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:45:33,275]\u001b[0m Trial 22 finished with values: [0.8300885322717755, 0.8306818181818182] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.6361634492421595e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:45:52,760]\u001b[0m Trial 23 finished with values: [0.33130699088145893, 0.4954545454545455] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 4.757543850277486e-10, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:45:58,380]\u001b[0m Trial 24 finished with values: [0.9208014134484586, 0.9208333333333333] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:46:16,024]\u001b[0m Trial 25 finished with values: [0.3579702244253927, 0.5053030303030303] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:46:31,386]\u001b[0m Trial 26 finished with values: [0.329608938547486, 0.49166666666666664] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 3.210169513747307e-09, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:46:33,117]\u001b[0m Trial 27 finished with values: [0.6380976627140718, 0.6393939393939394] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 9.38296135816313e-09, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:46:59,565]\u001b[0m Trial 28 finished with values: [0.9188886993765043, 0.918939393939394] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:47:19,706]\u001b[0m Trial 29 finished with values: [0.8969436134834402, 0.896969696969697] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.3611024252456128e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:47:31,678]\u001b[0m Trial 30 finished with values: [0.8765810246135429, 0.8767676767676768] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.0029474102733971132, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:47:50,451]\u001b[0m Trial 31 finished with values: [0.32687404385517593, 0.4856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 5.597132253721025e-05, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:47:58,664]\u001b[0m Trial 32 finished with values: [0.32938187976291283, 0.4911616161616162] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 1.0307939457429593e-07, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:48:04,300]\u001b[0m Trial 33 finished with values: [0.8855255789911984, 0.8856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:48:15,766]\u001b[0m Trial 34 finished with values: [0.5108559023066486, 0.5554924242424243] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 9.024765587404826e-06, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:48:16,593]\u001b[0m Trial 35 finished with values: [0.33784800601956355, 0.5102272727272728] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0162142687669362e-12, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:48:33,048]\u001b[0m Trial 36 finished with values: [0.33096806893056263, 0.4946969696969697] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'SVC', 'C': 4.0968821121087487e-07, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:48:59,272]\u001b[0m Trial 37 finished with values: [0.9097435897435897, 0.9098484848484848] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:49:17,933]\u001b[0m Trial 38 finished with values: [0.6696899710573792, 0.669949494949495] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 6.001573182888972e-13, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:49:34,746]\u001b[0m Trial 39 finished with values: [0.9598445900687107, 0.9598484848484848] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.5941361448034762e-13, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:49:53,482]\u001b[0m Trial 40 finished with values: [0.6761978724446477, 0.6762626262626262] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 8.049692801655839e-10, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:50:05,467]\u001b[0m Trial 41 finished with values: [0.331222292590247, 0.4952651515151515] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'SVC', 'C': 2.8602481903088166e-08, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:50:24,598]\u001b[0m Trial 42 finished with values: [0.9792826985267697, 0.9792929292929293] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:50:26,281]\u001b[0m Trial 43 finished with values: [0.6129770984971605, 0.6130681818181818] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.9139401917749614e-11, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:50:41,508]\u001b[0m Trial 44 finished with values: [0.8841011019175538, 0.8848484848484849] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0015578700560006545, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:51:02,884]\u001b[0m Trial 45 finished with values: [0.9863630024123022, 0.9863636363636363] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:51:04,612]\u001b[0m Trial 46 finished with values: [0.4936227782354197, 0.5111742424242425] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 6.334357949214483e-11, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:51:20,834]\u001b[0m Trial 47 finished with values: [0.33198380566801616, 0.49696969696969695] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'SVC', 'C': 1.5782536965185027e-10, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:51:40,842]\u001b[0m Trial 48 finished with values: [0.985983961622916, 0.9859848484848485] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:51:53,595]\u001b[0m Trial 49 finished with values: [0.33074193003211083, 0.4941919191919192] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 6.0425240683924734e-05, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:52:20,131]\u001b[0m Trial 50 finished with values: [0.9286501257018256, 0.9287878787878788] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:52:39,906]\u001b[0m Trial 51 finished with values: [0.9802947858694322, 0.9803030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:53:00,354]\u001b[0m Trial 52 finished with values: [0.9095940917052924, 0.9095959595959596] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 2.5937943078584336e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:53:12,330]\u001b[0m Trial 53 finished with values: [0.33173016073914696, 0.49640151515151515] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.0006947731606244794, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:53:18,124]\u001b[0m Trial 54 finished with values: [0.9264137508125004, 0.9265151515151515] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:53:34,032]\u001b[0m Trial 55 finished with values: [0.4492108132927739, 0.4492424242424242] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:53:54,316]\u001b[0m Trial 56 finished with values: [0.8977157669251132, 0.8977272727272727] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.3611024252456128e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:00,348]\u001b[0m Trial 57 finished with values: [0.7222930786633641, 0.7325757575757575] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:04,392]\u001b[0m Trial 58 finished with values: [0.9446637678165102, 0.9446969696969697] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:14,255]\u001b[0m Trial 59 finished with values: [0.45094597748613086, 0.45151515151515154] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:16,458]\u001b[0m Trial 60 finished with values: [0.46420551734909454, 0.4732323232323232] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:54:36,743]\u001b[0m Trial 61 finished with values: [0.8927457683373484, 0.8928030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:42,410]\u001b[0m Trial 62 finished with values: [0.8772568771793878, 0.8772727272727273] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:54:56,441]\u001b[0m Trial 63 finished with values: [0.32733140818753187, 0.4866161616161616] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 4.757543850277486e-10, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:55:18,678]\u001b[0m Trial 64 finished with values: [0.6816537467700259, 0.6818181818181818] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 7.3814401753163066e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:55:31,113]\u001b[0m Trial 65 finished with values: [0.9838374562360098, 0.9838383838383838] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:55:45,685]\u001b[0m Trial 66 finished with values: [0.3320964749536178, 0.49722222222222223] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 3.108824722327291e-08, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:55:47,179]\u001b[0m Trial 67 finished with values: [0.4768203918961771, 0.4837121212121212] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:56:05,687]\u001b[0m Trial 68 finished with values: [0.9802975574023087, 0.9803030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:56:18,345]\u001b[0m Trial 69 finished with values: [0.3301759133964817, 0.49292929292929294] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 6.0425240683924734e-05, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:56:31,600]\u001b[0m Trial 70 finished with values: [0.8733242500392199, 0.8734848484848485] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.004577850635519135, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:56:44,389]\u001b[0m Trial 71 finished with values: [0.37133850916623123, 0.5128787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.00031905157474440256, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:56:58,462]\u001b[0m Trial 72 finished with values: [0.9878697653902352, 0.9878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:57:14,352]\u001b[0m Trial 73 finished with values: [0.681771426472209, 0.6818181818181818] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:57:26,075]\u001b[0m Trial 74 finished with values: [0.332827899924185, 0.49886363636363634] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.00013682391388003578, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:57:27,804]\u001b[0m Trial 75 finished with values: [0.42560462065814453, 0.47632575757575757] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.049692801655839e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:57:44,411]\u001b[0m Trial 76 finished with values: [0.3302891933028919, 0.49318181818181817] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'SVC', 'C': 1.5782536965185027e-10, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:57:46,067]\u001b[0m Trial 77 finished with values: [0.5736279780989495, 0.5736742424242425] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:58:05,623]\u001b[0m Trial 78 finished with values: [0.33433581206499174, 0.49924242424242427] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0015578700560006545, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:58:19,397]\u001b[0m Trial 79 finished with values: [0.9848340903096731, 0.9848484848484849] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:58:35,242]\u001b[0m Trial 80 finished with values: [0.9780244730329772, 0.978030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 9.38296135816313e-09, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:58:45,317]\u001b[0m Trial 81 finished with values: [0.4464395499424846, 0.446780303030303] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:59:01,523]\u001b[0m Trial 82 finished with values: [0.3316455696202532, 0.4962121212121212] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 2.2843091500720636e-09, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:59:04,583]\u001b[0m Trial 83 finished with values: [0.5298574483871651, 0.553219696969697] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.6361634492421595e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:59:26,013]\u001b[0m Trial 84 finished with values: [0.9844564741786337, 0.984469696969697] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 18:59:50,190]\u001b[0m Trial 85 finished with values: [0.9886193749967669, 0.9886363636363636] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 18:59:58,491]\u001b[0m Trial 86 finished with values: [0.7891694183867177, 0.7929924242424242] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:00:15,398]\u001b[0m Trial 87 finished with values: [0.33113757284013173, 0.49507575757575756] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 1.3544219733466798e-09, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:00:17,252]\u001b[0m Trial 88 finished with values: [0.6142335082832195, 0.6145833333333334] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 2.0019729092786205e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:00:30,282]\u001b[0m Trial 89 finished with values: [0.3326592517694641, 0.4984848484848485] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 1.0307939457429593e-07, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:00:42,053]\u001b[0m Trial 90 finished with values: [0.6589864249332466, 0.659280303030303] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.6361634492421595e-13, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:00:44,545]\u001b[0m Trial 91 finished with values: [0.6459414057838933, 0.6462121212121212] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 2.6133268485069104e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:01:03,430]\u001b[0m Trial 92 finished with values: [0.651567148346714, 0.6517676767676768] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.5391091063127621e-12, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:01:15,771]\u001b[0m Trial 93 finished with values: [0.6647553628165142, 0.6647727272727273] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:01:26,965]\u001b[0m Trial 94 finished with values: [0.5478822185037511, 0.5488636363636363] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.014849722376739682, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:01:36,954]\u001b[0m Trial 95 finished with values: [0.7469776233741781, 0.7541666666666667] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.6273740496827498e-11, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:01:55,812]\u001b[0m Trial 96 finished with values: [0.9797879985627023, 0.9797979797979798] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:02:07,836]\u001b[0m Trial 97 finished with values: [0.32881355932203393, 0.4898989898989899] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 5.704657672805473e-07, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:02:16,609]\u001b[0m Trial 98 finished with values: [0.8848605067882239, 0.8852272727272728] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.008834950627539999, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:02:18,865]\u001b[0m Trial 99 finished with values: [0.6204177287627186, 0.6204545454545455] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 4.946263295005424e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:02:31,259]\u001b[0m Trial 100 finished with values: [0.6784068299378252, 0.678409090909091] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:02:54,704]\u001b[0m Trial 101 finished with values: [0.9238470172338746, 0.9238636363636363] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.728767345094246e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:03:09,399]\u001b[0m Trial 102 finished with values: [0.7074246339362619, 0.7075757575757575] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:03:25,625]\u001b[0m Trial 103 finished with values: [0.9804844096507006, 0.9804924242424242] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 9.212533433451448e-11, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:03:57,811]\u001b[0m Trial 104 finished with values: [0.3641773572020877, 0.5212121212121212] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:04:01,265]\u001b[0m Trial 105 finished with values: [0.3330807123910572, 0.4994318181818182] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:04:12,342]\u001b[0m Trial 106 finished with values: [0.5513921910779864, 0.553219696969697] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.013001932466376319, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:04:32,655]\u001b[0m Trial 107 finished with values: [0.8934552934912853, 0.893560606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:04:48,771]\u001b[0m Trial 108 finished with values: [0.9791579465926687, 0.9791666666666666] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:04:57,876]\u001b[0m Trial 109 finished with values: [0.8873541338179676, 0.8875] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.008834950627539999, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:05:11,258]\u001b[0m Trial 110 finished with values: [0.9856057953317806, 0.9856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:05:32,736]\u001b[0m Trial 111 finished with values: [0.9806608157878067, 0.9806818181818182] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:05:45,652]\u001b[0m Trial 112 finished with values: [0.9863542245238017, 0.9863636363636363] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:06:03,314]\u001b[0m Trial 113 finished with values: [0.9033890745270341, 0.9034090909090909] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:06:23,165]\u001b[0m Trial 114 finished with values: [0.9812662970557707, 0.9813131313131314] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:06:44,651]\u001b[0m Trial 115 finished with values: [0.9856033835492826, 0.9856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:07:06,141]\u001b[0m Trial 116 finished with values: [0.6764408725602755, 0.6765151515151515] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:07:18,552]\u001b[0m Trial 117 finished with values: [0.9835202297467099, 0.9835227272727273] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 2.0033264426836306e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:07:42,373]\u001b[0m Trial 118 finished with values: [0.9117068389162241, 0.9117424242424242] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.5937943078584336e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:08:06,356]\u001b[0m Trial 119 finished with values: [0.9123767798466593, 0.9125] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:08:25,994]\u001b[0m Trial 120 finished with values: [0.979538501216265, 0.9795454545454545] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:08:47,781]\u001b[0m Trial 121 finished with values: [0.9840899959823222, 0.9840909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.558450390218454e-09, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:09:14,466]\u001b[0m Trial 122 finished with values: [0.9248498636221698, 0.925] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:09:29,383]\u001b[0m Trial 123 finished with values: [0.9882531919410351, 0.9882575757575758] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.340640463015051e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:09:31,528]\u001b[0m Trial 124 finished with values: [0.6113455687506134, 0.6113636363636363] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:09:37,051]\u001b[0m Trial 125 finished with values: [0.881367478420042, 0.881439393939394] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.11735489871617412, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:09:53,938]\u001b[0m Trial 126 finished with values: [0.9526227948961828, 0.9526515151515151] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:10:12,992]\u001b[0m Trial 127 finished with values: [0.9840835759001556, 0.9840909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:10:31,833]\u001b[0m Trial 128 finished with values: [0.32790224032586557, 0.48787878787878786] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0002801196885179988, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:10:33,630]\u001b[0m Trial 129 finished with values: [0.6054770131882519, 0.6054924242424242] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:10:55,501]\u001b[0m Trial 130 finished with values: [0.9818181818181818, 0.9818181818181818] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:11:07,546]\u001b[0m Trial 131 finished with values: [0.33181473044798787, 0.4965909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.00035604895832598257, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:11:19,418]\u001b[0m Trial 132 finished with values: [0.9874924178525923, 0.9875] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:11:32,690]\u001b[0m Trial 133 finished with values: [0.8961214420990776, 0.8962121212121212] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.004577850635519135, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:11:52,287]\u001b[0m Trial 134 finished with values: [0.9833325585039249, 0.9833333333333333] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.3611024252456128e-09, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:12:05,752]\u001b[0m Trial 135 finished with values: [0.9833332270509837, 0.9833333333333333] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:12:27,866]\u001b[0m Trial 136 finished with values: [0.9833268646467523, 0.9833333333333333] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:12:38,903]\u001b[0m Trial 137 finished with values: [0.3323216995447648, 0.49772727272727274] and parameters: {'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0009045007277451313, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:12:46,125]\u001b[0m Trial 138 finished with values: [0.7772516097051642, 0.7818181818181819] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.6273740496827498e-11, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:13:03,020]\u001b[0m Trial 139 finished with values: [0.9473288283807706, 0.9473484848484849] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:13:08,190]\u001b[0m Trial 140 finished with values: [0.8765133797259488, 0.8765151515151515] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:13:31,684]\u001b[0m Trial 141 finished with values: [0.9139629955314682, 0.9140151515151516] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:13:38,637]\u001b[0m Trial 142 finished with values: [0.32627280847262985, 0.484280303030303] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:13:40,745]\u001b[0m Trial 143 finished with values: [0.47269392231761276, 0.48787878787878786] and parameters: {'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 6.001573182888972e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:13:49,019]\u001b[0m Trial 144 finished with values: [0.8875977823346244, 0.8878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.01174839762618421, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:14:15,392]\u001b[0m Trial 145 finished with values: [0.9158835528004083, 0.9159090909090909] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:14:21,064]\u001b[0m Trial 146 finished with values: [0.8760565337723979, 0.8761363636363636] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:14:23,687]\u001b[0m Trial 147 finished with values: [0.4927275588386182, 0.4994318181818182] and parameters: {'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.6361634492421595e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:14:42,864]\u001b[0m Trial 148 finished with values: [0.9787769164513351, 0.9787878787878788] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:15:01,647]\u001b[0m Trial 149 finished with values: [0.8877540701789868, 0.8878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 2.2310715680694646e-10, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:15:17,052]\u001b[0m Trial 150 finished with values: [0.9859791975961385, 0.9859848484848485] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:15:30,730]\u001b[0m Trial 151 finished with values: [0.9878767770825165, 0.9878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.0033264426836306e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:15:56,963]\u001b[0m Trial 152 finished with values: [0.9097240860483253, 0.9098484848484848] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:16:12,690]\u001b[0m Trial 153 finished with values: [0.9893851424160058, 0.9893939393939394] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:16:32,413]\u001b[0m Trial 154 finished with values: [0.9784033614289604, 0.9784090909090909] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:16:51,325]\u001b[0m Trial 155 finished with values: [0.9835855560801006, 0.9835858585858586] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.558450390218454e-09, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:17:12,729]\u001b[0m Trial 156 finished with values: [0.6761352019312512, 0.6761363636363636] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'sgd', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:17:20,731]\u001b[0m Trial 157 finished with values: [0.87552818363605, 0.8757575757575757] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.01752956397182444, 'kernel': 'rbf', 'gamma': 'auto'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:17:33,607]\u001b[0m Trial 158 finished with values: [0.985412521973519, 0.9854166666666667] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:17:46,620]\u001b[0m Trial 159 finished with values: [0.9882535291442085, 0.9882575757575758] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.0033264426836306e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:18:00,974]\u001b[0m Trial 160 finished with values: [0.9840788465957814, 0.9840909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:18:22,794]\u001b[0m Trial 161 finished with values: [0.9840885713236105, 0.9840909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:18:41,891]\u001b[0m Trial 162 finished with values: [0.9821880734451995, 0.9821969696969697] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.340640463015051e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:19:00,624]\u001b[0m Trial 163 finished with values: [0.9772434660525382, 0.9772727272727273] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:19:22,211]\u001b[0m Trial 164 finished with values: [0.9874999982064965, 0.9875] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:19:41,123]\u001b[0m Trial 165 finished with values: [0.9828125000000001, 0.9828282828282828] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:19:57,968]\u001b[0m Trial 166 finished with values: [0.9460731878675029, 0.9462121212121212] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:20:13,100]\u001b[0m Trial 167 finished with values: [0.9878770067227509, 0.9878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.4956248692585555e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:20:33,189]\u001b[0m Trial 168 finished with values: [0.9780090870496616, 0.978030303030303] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 2.742564209260707e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:20:52,282]\u001b[0m Trial 169 finished with values: [0.9613519026380255, 0.9613636363636363] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:21:05,369]\u001b[0m Trial 170 finished with values: [0.9878738389302848, 0.9878787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:21:27,098]\u001b[0m Trial 171 finished with values: [0.985605060957516, 0.9856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:21:37,669]\u001b[0m Trial 172 finished with values: [0.8751500503605737, 0.8753787878787879] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.006202390109100647, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:21:53,874]\u001b[0m Trial 173 finished with values: [0.9778067982200174, 0.977840909090909] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 9.212533433451448e-11, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:22:08,471]\u001b[0m Trial 174 finished with values: [0.9845952434657488, 0.9845959595959596] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:22:27,547]\u001b[0m Trial 175 finished with values: [0.9785315079459889, 0.9785353535353535] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:22:30,685]\u001b[0m Trial 176 finished with values: [0.8526164901277347, 0.8527777777777777] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.8100172356068487, 'kernel': 'sigmoid', 'gamma': 'auto'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:22:45,556]\u001b[0m Trial 177 finished with values: [0.9540318562450777, 0.954040404040404] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.1434710246970276e-11, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:23:11,912]\u001b[0m Trial 178 finished with values: [0.9119141416000883, 0.9121212121212121] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 1.6699461276318065e-13, 'solver': 'sgd', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:23:27,558]\u001b[0m Trial 179 finished with values: [0.3273885350318471, 0.48674242424242425] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 8.38735443263675e-07, 'kernel': 'sigmoid', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:23:47,791]\u001b[0m Trial 180 finished with values: [0.9855742351698766, 0.9856060606060606] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 3.558450390218454e-09, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:24:04,035]\u001b[0m Trial 181 finished with values: [0.979914210480757, 0.9799242424242425] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:24:06,088]\u001b[0m Trial 182 finished with values: [0.5892408313968315, 0.5897727272727272] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:24:25,125]\u001b[0m Trial 183 finished with values: [0.9830773019432975, 0.9830808080808081] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:24:44,670]\u001b[0m Trial 184 finished with values: [0.9848352470217632, 0.9848484848484849] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 7.16730660770492e-13, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:25:05,446]\u001b[0m Trial 185 finished with values: [0.9837120066988534, 0.9837121212121213] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:25:24,597]\u001b[0m Trial 186 finished with values: [0.9598482774532238, 0.9598484848484848] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 8.990079895619952e-09, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:25:43,815]\u001b[0m Trial 187 finished with values: [0.9787864056216025, 0.9787878787878788] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:26:04,016]\u001b[0m Trial 188 finished with values: [0.9844678227500765, 0.984469696969697] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0460665190913303e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:26:19,000]\u001b[0m Trial 189 finished with values: [0.8615056818181819, 0.8621212121212121] and parameters: {'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.0027072840135378413, 'kernel': 'rbf', 'gamma': 'scale'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:26:39,056]\u001b[0m Trial 190 finished with values: [0.9795392425730722, 0.9795454545454545] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:26:50,520]\u001b[0m Trial 191 finished with values: [0.9840908520246932, 0.9840909090909091] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.040837028410876e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:27:09,425]\u001b[0m Trial 192 finished with values: [0.9810358766095799, 0.9810606060606061] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.589760146693692e-11, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:27:24,385]\u001b[0m Trial 193 finished with values: [0.9852257273810923, 0.9852272727272727] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.340640463015051e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:27:40,927]\u001b[0m Trial 194 finished with values: [0.9787847617648139, 0.9787878787878788] and parameters: {'StandardScaler': 1, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'constant'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:27:44,540]\u001b[0m Trial 195 finished with values: [0.4999997117289976, 0.5022727272727273] and parameters: {'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.9128281509102777e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:28:03,884]\u001b[0m Trial 196 finished with values: [0.9810605951908833, 0.9810606060606061] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:696: ConvergenceWarning:\n",
            "\n",
            "Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "\n",
            "\u001b[32m[I 2021-12-02 19:28:18,773]\u001b[0m Trial 197 finished with values: [0.9428428515628293, 0.942929292929293] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.5941361448034762e-13, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:28:38,136]\u001b[0m Trial 198 finished with values: [0.9821801945960664, 0.9821969696969697] and parameters: {'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 9.4094080053587e-10, 'solver': 'adam', 'activation': 'tanh', 'learning_rate': 'adaptive'}. \u001b[0m\n",
            "\u001b[32m[I 2021-12-02 19:28:52,066]\u001b[0m Trial 199 finished with values: [0.9888880697510861, 0.9888888888888889] and parameters: {'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}. \u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMkaHF8whRYw"
      },
      "source": [
        "#Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXUCuSP15THE"
      },
      "source": [
        "trialsFile = 'drive/MyDrive/NEURAL/first/kaggle/study' \n",
        "with open(trialsFile, 'rb') as handle:\n",
        "    trials = pickle.load(handle)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7DbVR3go0r3"
      },
      "source": [
        "Χωρίζουμε τα δεδομένα με βάση τον classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mcZzvseS8dwM"
      },
      "source": [
        "MLPs = []\n",
        "SVCs =[]\n",
        "\n",
        "for trial in trials:\n",
        "  if trial.params['classifier'] == 'MLP':\n",
        "    MLPs.append(trial)\n",
        "  elif trial.params['classifier'] =='SVC':\n",
        "    SVCs.append(trial)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syw06Im3pBFn"
      },
      "source": [
        "Κάνουμε sort με βάση το f1 score, για να δούμε τις ακραίες περιπρώσεις."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QgufY9EVrfIy"
      },
      "source": [
        "###MLP outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnJqnhEM9M16"
      },
      "source": [
        "MLPs.sort(key = lambda key:key.values[0])"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "syqDzzoQ9a2V"
      },
      "source": [
        "MLPsWorse = MLPs[:5]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJ9WUe5q-MCn"
      },
      "source": [
        "MLPsBest = MLPs[-5:]"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9AbP44dpT5j",
        "outputId": "762b5432-5bf0-4079-a018-d80f32f9c4c1"
      },
      "source": [
        "MLPsWorse"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FrozenTrial(number=105, values=[0.3330807123910572, 0.4994318181818182], datetime_start=datetime.datetime(2021, 12, 2, 19, 3, 57, 813600), datetime_complete=datetime.datetime(2021, 12, 2, 19, 4, 1, 265050), params={'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.253855370372395e-12, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'adaptive'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 2, 'nsga2:parents': [86, 56]}, intermediate_values={}, trial_id=105, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=35, values=[0.33784800601956355, 0.5102272727272728], datetime_start=datetime.datetime(2021, 12, 2, 18, 48, 15, 768576), datetime_complete=datetime.datetime(2021, 12, 2, 18, 48, 16, 593478), params={'StandardScaler': 0, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.0162142687669362e-12, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=35, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=8, values=[0.3469677685541632, 0.5145833333333333], datetime_start=datetime.datetime(2021, 12, 2, 18, 42, 17, 116127), datetime_complete=datetime.datetime(2021, 12, 2, 18, 42, 17, 768460), params={'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 4.738979631252411e-09, 'solver': 'sgd', 'activation': 'relu', 'learning_rate': 'constant'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=8, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=75, values=[0.42560462065814453, 0.47632575757575757], datetime_start=datetime.datetime(2021, 12, 2, 18, 57, 26, 77367), datetime_complete=datetime.datetime(2021, 12, 2, 18, 57, 27, 804707), params={'StandardScaler': 0, 'test size': 0.4, 'classifier': 'MLP', 'alpha': 8.049692801655839e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [40, 0]}, intermediate_values={}, trial_id=75, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=60, values=[0.46420551734909454, 0.4732323232323232], datetime_start=datetime.datetime(2021, 12, 2, 18, 54, 14, 256885), datetime_complete=datetime.datetime(2021, 12, 2, 18, 54, 16, 458262), params={'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'adaptive'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [45, 42]}, intermediate_values={}, trial_id=60, state=TrialState.COMPLETE, value=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeYlHcfR_XaQ",
        "outputId": "c7c65de7-c8f3-4ba4-866d-d955de89f41d"
      },
      "source": [
        "MLPsBest"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FrozenTrial(number=123, values=[0.9882531919410351, 0.9882575757575758], datetime_start=datetime.datetime(2021, 12, 2, 19, 9, 14, 468997), datetime_complete=datetime.datetime(2021, 12, 2, 19, 9, 29, 383467), params={'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 1.340640463015051e-12, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 2, 'nsga2:parents': [45, 54]}, intermediate_values={}, trial_id=123, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=159, values=[0.9882535291442085, 0.9882575757575758], datetime_start=datetime.datetime(2021, 12, 2, 19, 17, 33, 613441), datetime_complete=datetime.datetime(2021, 12, 2, 19, 17, 46, 619884), params={'StandardScaler': 1, 'test size': 0.2, 'classifier': 'MLP', 'alpha': 2.0033264426836306e-09, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 3, 'nsga2:parents': [117, 115]}, intermediate_values={}, trial_id=159, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=85, values=[0.9886193749967669, 0.9886363636363636], datetime_start=datetime.datetime(2021, 12, 2, 18, 59, 26, 19860), datetime_complete=datetime.datetime(2021, 12, 2, 18, 59, 50, 189973), params={'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 9.882292915479614e-12, 'solver': 'adam', 'activation': 'logistic', 'learning_rate': 'constant'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [37, 43]}, intermediate_values={}, trial_id=85, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=199, values=[0.9888880697510861, 0.9888888888888889], datetime_start=datetime.datetime(2021, 12, 2, 19, 28, 38, 138651), datetime_complete=datetime.datetime(2021, 12, 2, 19, 28, 52, 66539), params={'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'MLP', 'alpha': 1.0996557591532293e-10, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'constant'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 3, 'nsga2:parents': [114, 123]}, intermediate_values={}, trial_id=199, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=153, values=[0.9893851424160058, 0.9893939393939394], datetime_start=datetime.datetime(2021, 12, 2, 19, 15, 56, 966042), datetime_complete=datetime.datetime(2021, 12, 2, 19, 16, 12, 690478), params={'StandardScaler': 1, 'test size': 0.1, 'classifier': 'MLP', 'alpha': 3.101331211644446e-13, 'solver': 'adam', 'activation': 'relu', 'learning_rate': 'adaptive'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'alpha': LogUniformDistribution(high=1e-08, low=1e-13), 'solver': CategoricalDistribution(choices=('sgd', 'adam')), 'activation': CategoricalDistribution(choices=('logistic', 'tanh', 'relu')), 'learning_rate': CategoricalDistribution(choices=('constant', 'adaptive'))}, user_attrs={}, system_attrs={'nsga2:generation': 3, 'nsga2:parents': [132, 85]}, intermediate_values={}, trial_id=153, state=TrialState.COMPLETE, value=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfLcZJxyrkEa"
      },
      "source": [
        "###SVC outliers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HC18GwtIrn06"
      },
      "source": [
        "SVCs.sort(key = lambda key:key.values[0])"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "losZHovzrnrp"
      },
      "source": [
        "SVCsWorse = SVCs[:5]"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SGtRar9rngw"
      },
      "source": [
        "SVCsBest = SVCs[-5:]"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_UHg6-crnYK",
        "outputId": "cb833b66-21e4-4fe3-b76d-1ead17033a9b"
      },
      "source": [
        "SVCsWorse"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FrozenTrial(number=4, values=[0.31712364200724263, 0.4643939393939394], datetime_start=datetime.datetime(2021, 12, 2, 18, 41, 22, 611361), datetime_complete=datetime.datetime(2021, 12, 2, 18, 41, 33, 545491), params={'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.01192107798646013, 'kernel': 'sigmoid', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=4, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=14, values=[0.32342388518708354, 0.478030303030303], datetime_start=datetime.datetime(2021, 12, 2, 18, 43, 21, 71674), datetime_complete=datetime.datetime(2021, 12, 2, 18, 43, 35, 598455), params={'StandardScaler': 0, 'test size': 0.1, 'classifier': 'SVC', 'C': 4.0327625186680925e-08, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=14, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=142, values=[0.32627280847262985, 0.484280303030303], datetime_start=datetime.datetime(2021, 12, 2, 19, 13, 31, 686543), datetime_complete=datetime.datetime(2021, 12, 2, 19, 13, 38, 637699), params={'StandardScaler': 0, 'test size': 0.4, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'sigmoid', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 2, 'nsga2:parents': [58, 86]}, intermediate_values={}, trial_id=142, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=31, values=[0.32687404385517593, 0.4856060606060606], datetime_start=datetime.datetime(2021, 12, 2, 18, 47, 31, 684209), datetime_complete=datetime.datetime(2021, 12, 2, 18, 47, 50, 451140), params={'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 5.597132253721025e-05, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=31, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=63, values=[0.32733140818753187, 0.4866161616161616], datetime_start=datetime.datetime(2021, 12, 2, 18, 54, 42, 415377), datetime_complete=datetime.datetime(2021, 12, 2, 18, 54, 56, 441244), params={'StandardScaler': 0, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 4.757543850277486e-10, 'kernel': 'rbf', 'gamma': 'scale'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [23, 22]}, intermediate_values={}, trial_id=63, state=TrialState.COMPLETE, value=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiMkgPn_rqat",
        "outputId": "23c84ce7-19b3-4801-c8bf-15498a0435e8"
      },
      "source": [
        "SVCsBest"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[FrozenTrial(number=144, values=[0.8875977823346244, 0.8878787878787879], datetime_start=datetime.datetime(2021, 12, 2, 19, 13, 40, 753894), datetime_complete=datetime.datetime(2021, 12, 2, 19, 13, 49, 19652), params={'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.01174839762618421, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 2, 'nsga2:parents': [96, 80]}, intermediate_values={}, trial_id=144, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=133, values=[0.8961214420990776, 0.8962121212121212], datetime_start=datetime.datetime(2021, 12, 2, 19, 11, 19, 427195), datetime_complete=datetime.datetime(2021, 12, 2, 19, 11, 32, 690855), params={'StandardScaler': 1, 'test size': 0.1, 'classifier': 'SVC', 'C': 0.004577850635519135, 'kernel': 'rbf', 'gamma': 'scale'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 2, 'nsga2:parents': [70, 50]}, intermediate_values={}, trial_id=133, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=24, values=[0.9208014134484586, 0.9208333333333333], datetime_start=datetime.datetime(2021, 12, 2, 18, 45, 52, 762057), datetime_complete=datetime.datetime(2021, 12, 2, 18, 45, 58, 380524), params={'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 0}, intermediate_values={}, trial_id=24, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=54, values=[0.9264137508125004, 0.9265151515151515], datetime_start=datetime.datetime(2021, 12, 2, 18, 53, 12, 332003), datetime_complete=datetime.datetime(2021, 12, 2, 18, 53, 18, 123944), params={'StandardScaler': 1, 'test size': 0.2, 'classifier': 'SVC', 'C': 0.15217650086812523, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [24, 24]}, intermediate_values={}, trial_id=54, state=TrialState.COMPLETE, value=None),\n",
              " FrozenTrial(number=58, values=[0.9446637678165102, 0.9446969696969697], datetime_start=datetime.datetime(2021, 12, 2, 18, 54, 0, 351453), datetime_complete=datetime.datetime(2021, 12, 2, 18, 54, 4, 391997), params={'StandardScaler': 1, 'test size': 0.30000000000000004, 'classifier': 'SVC', 'C': 0.7214566634713006, 'kernel': 'rbf', 'gamma': 'auto'}, distributions={'StandardScaler': IntUniformDistribution(high=1, low=0, step=1), 'test size': DiscreteUniformDistribution(high=0.4, low=0.1, q=0.1), 'classifier': CategoricalDistribution(choices=('SVC', 'MLP')), 'C': LogUniformDistribution(high=1.0, low=1e-10), 'kernel': CategoricalDistribution(choices=('rbf', 'sigmoid')), 'gamma': CategoricalDistribution(choices=('auto', 'scale'))}, user_attrs={}, system_attrs={'nsga2:generation': 1, 'nsga2:parents': [42, 25]}, intermediate_values={}, trial_id=58, state=TrialState.COMPLETE, value=None)]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7dSkdImmPJd"
      },
      "source": [
        "##Graph Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tysaHok4mR49"
      },
      "source": [
        "###Discreet parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw4XkWirpfek"
      },
      "source": [
        "Θα κρίνουμε την επίδραση κάθε παραμέτρου ξεχωριστά με βάση την μέση τιμή των F1 σε όλα τα trials."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5pixjM1_zhK"
      },
      "source": [
        "#υπολογίζει τον μέσο όρο για μία μεταβλητή\n",
        "def meansDicreetParams(trials,name,value):\n",
        "  sum = 0\n",
        "  counter = 0\n",
        "  for clf in trials:\n",
        "    if clf.params[name] ==value:\n",
        "        sum = sum + clf.values[0]\n",
        "        counter = counter +1\n",
        "\n",
        "  return (sum/counter)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OFEOXtnDe0z"
      },
      "source": [
        "# του παρέχουμε σε ένα λεξικό με  τις παραμέτους ως κλειδιά και μία λίστα με τις δυνατές τιμές του.\n",
        "def BarPlotDicreetParams(DiscreetDict,list):\n",
        "  fig, axes = plt.subplots(1,len(DiscreetDict),figsize=(5*len(DiscreetDict),6) )\n",
        "\n",
        "  for i,key in enumerate(DiscreetDict):\n",
        "    axes[i].tick_params(axis='x', rotation=45)\n",
        "    axes[i].set_title(key)\n",
        "    f1_averages =[]\n",
        "    for paramValue in DiscreetDict[key]:  \n",
        "      f1_averages.append(meansDicreetParams(list,key,paramValue))\n",
        "      print(\"for parameter :\",key,\" value :\",paramValue, \" mean f1 score: \",meansDicreetParams(list,key,paramValue))\n",
        "\n",
        "    axes[i].bar(DiscreetDict[key], f1_averages)\n",
        "      \n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "718dIghSmUWv"
      },
      "source": [
        "###Continuous parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rsy-WK3lw_N"
      },
      "source": [
        "#επειδή δεν μπορούμε να έχουμε λίστα με όλες τις τιμές θα κάνουμε scatterplot\n",
        "def ContinuousVariable(ClfList,paramName,logUniform=True): \n",
        "  values = []\n",
        "  f1_scores = []\n",
        "\n",
        "  for clf in ClfList:\n",
        "    values.append(clf.params[paramName])\n",
        "    f1_scores.append(clf.values[0])\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(20,6))\n",
        "  ax.scatter(values, f1_scores)\n",
        "  if logUniform:\n",
        "    ax.set_xscale('log')\n",
        "  ax.set_title('Simple plot')"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AHtdZxMmaC-"
      },
      "source": [
        "##MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "91E-eOHElsvr",
        "outputId": "08431a9a-0427-43bc-c437-30c7d72eaeb7"
      },
      "source": [
        "DicreetParamsMLP ={'StandardScaler':[0,1],\n",
        "                'solver':['sgd','adam'],\n",
        "                'activation':['logistic','tanh','relu'],\n",
        "                'learning_rate':['constant','adaptive']}\n",
        "\n",
        "BarPlotDicreetParams(DicreetParamsMLP,MLPs)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for parameter : StandardScaler  value : 0  mean f1 score:  0.6066255508206432\n",
            "for parameter : StandardScaler  value : 1  mean f1 score:  0.9657853735239658\n",
            "for parameter : solver  value : sgd  mean f1 score:  0.8060667006548186\n",
            "for parameter : solver  value : adam  mean f1 score:  0.8774325819092751\n",
            "for parameter : activation  value : logistic  mean f1 score:  0.801617264197081\n",
            "for parameter : activation  value : tanh  mean f1 score:  0.8660551829687806\n",
            "for parameter : activation  value : relu  mean f1 score:  0.8980916924341315\n",
            "for parameter : learning_rate  value : constant  mean f1 score:  0.8583993045201831\n",
            "for parameter : learning_rate  value : adaptive  mean f1 score:  0.8508960675041652\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 4 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "djohGZKwqM7t"
      },
      "source": [
        "Βλέπουμε ότι το Standard Scaler είναι αραπαίτητο, ενώ με σχετικά μιρκή διαφορά υπερτερούν τα \n",
        "\n",
        "solver = adam\n",
        "\n",
        "activation = relu\n",
        "\n",
        "ενώ το learning rate είναι σχετικά το ίδιο."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "VYVeJiCgKrv1",
        "outputId": "8d82091c-bb28-4b58-81b2-8e78be652b54"
      },
      "source": [
        "ContinuousVariable(MLPs,'alpha')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P2nupNv1qkiU"
      },
      "source": [
        "Τέλος η τιμή για το alpha δεν παίζει ιδιαίτερο ρόλο."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1orCawsmbsu"
      },
      "source": [
        "##SVC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "cK2UwS7HMjB-",
        "outputId": "552b05da-40aa-4c9a-872e-509df6b21dc5"
      },
      "source": [
        "DicreetParamsSVC = {\n",
        "    'StandardScaler':[0,1],\n",
        "    'kernel':['rbf','sigmoid'],\n",
        "    'gamma':['auto','scale']\n",
        "}\n",
        "BarPlotDicreetParams(DicreetParamsSVC,SVCs)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "for parameter : StandardScaler  value : 0  mean f1 score:  0.36825891303847297\n",
            "for parameter : StandardScaler  value : 1  mean f1 score:  0.6311207241813721\n",
            "for parameter : kernel  value : rbf  mean f1 score:  0.5265838114688783\n",
            "for parameter : kernel  value : sigmoid  mean f1 score:  0.5149579118329163\n",
            "for parameter : gamma  value : auto  mean f1 score:  0.5688786527580704\n",
            "for parameter : gamma  value : scale  mean f1 score:  0.4684342550236522\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXT_nJSMq2H9"
      },
      "source": [
        "Ξαναβλέπουμε την σημασία του Standard Scaler, ενώ για kernel έχουμε rbf και gamma auto."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "XQO5IO8slMiK",
        "outputId": "4d01286e-0fd6-4ef2-8d80-fa1028f2926e"
      },
      "source": [
        "ContinuousVariable(SVCs,'C')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1440x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEmxwAPmrB4L"
      },
      "source": [
        "Ενώ βλέπουμε ότι θέλουμε το C να είναι μεγάλο"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_cs4RG3rLRg"
      },
      "source": [
        "#Συμπεράσματα"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wyKv6dVkrNWe"
      },
      "source": [
        "Σε κάθε περίπρωση, ο ταξινομητής MLP είναι αρκεα καλύτερος για αυτό το dataset. Βλέπουμε ότι και στους δύο, είναι πολύ χρήσιμο το preprocessing, με το SMOTE και Standard Scaler, να κάνουν την μεγαλύτερη διαφορά."
      ]
    }
  ]
}